{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firebase signin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cred = credentials.Certificate(\"../codeinterface-85b5e-firebase-adminsdk-11q7e-837ba92a03.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.collection('tasks').get()\n",
    "# save docs in a pickle file called old_tasks.pkl\n",
    "tasks = {}\n",
    "for doc in docs:\n",
    "    doc_dict = doc.to_dict()\n",
    "    doc_dict['id'] = doc.id \n",
    "    tasks[doc.id] = doc_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date_string = str(np.datetime64('today', 'D'))\n",
    "with open('old_tasks'+today_date_string+'.pkl', 'wb') as f:\n",
    "    pickle.dump(tasks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the documents from the collection\n",
    "for doc in docs:\n",
    "    doc.reference.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = './tasks'  # Replace with the actual folder path\n",
    "\n",
    "# Initialize a list to store all tasks\n",
    "all_tasks = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Load JSON content\n",
    "            task_data = json.load(file)\n",
    "            # Add the task to the list\n",
    "            all_tasks.append(task_data)\n",
    "\n",
    "all_tasks = np.array(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find task in all_tasks with field name  is Average\n",
    "def get_task_index(task_name):\n",
    "    for i in range(len(all_tasks)):\n",
    "        if all_tasks[i]['name'] == task_name:\n",
    "            return i\n",
    "    # raise error if task not found\n",
    "    raise ValueError('Task not found')\n",
    "tutorial_task_index = get_task_index('sum_product')\n",
    "tasks_chosen = np.array([ get_task_index('even_odd_count'), get_task_index('is_bored'),   get_task_index('table_transform_named'), get_task_index('is_multiply_prime'),\n",
    "                          get_task_index('tokenizer'), get_task_index('t_test')])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'name': 'even_odd_count', 'task_description': 'Given an integer. return a tuple that has the number of even and odd digits respectively. even_odd_count(-12) ==> (1, 1) , even_odd_count(123) ==> (1, 2)    ', 'function_signature': 'def even_odd_count(num):', 'unit_test': 'assert even_odd_count(7) == (0, 1)\\nassert even_odd_count(-78) == (1, 1)\\nassert even_odd_count(3452) == (2, 2)\\nassert even_odd_count(346211) == (3, 3)\\nassert even_odd_count(-345821) == (3, 3)\\nassert even_odd_count(-2) == (1, 0)\\nassert even_odd_count(-45347) == (2, 3)\\nassert even_odd_count(0) == (1, 0)', 'solution': 'def even_odd_count(num):\\n    even_count = 0\\n    odd_count = 0\\n    for i in str(abs(num)):\\n        if int(i)%2==0:\\n            even_count +=1\\n        else:\\n            odd_count +=1\\n    return (even_count, odd_count)', 'type': 'logic'},\n",
       "       {'name': 'is_bored', 'task_description': \"You'll be given a string of words, and your task is to count the number of boredoms. A boredom is a sentence that starts with the word 'I'. Sentences are delimited by '.', '?' or '!'.\", 'function_signature': 'def is_bored(S):', 'unit_test': 'assert is_bored(\"Hello world\") == 0, \"Test 1\"\\nassert is_bored(\"Is the sky blue?\") == 0, \"Test 2\"\\nassert is_bored(\"I love It !\") == 1, \"Test 3\"\\nassert is_bored(\"bIt\") == 0, \"Test 4\"\\nassert is_bored(\"I feel good today. I will be productive. will kill It\") == 2, \"Test 5\"\\nassert is_bored(\"You and I are going for a walk\") == 0, \"Test 6\"', 'solution': \"def is_bored(S):\\n    import re\\n    sentences = re.split(r'[.?!]\\\\s*', S)\\n    return sum(sentence[0:2] == 'I ' for sentence in sentences)\", 'type': 'logic'},\n",
       "       {'name': 'table_transform_named', 'task_description': '\\n+----+-------+---------+---------------------+----------+\\n|    |   age | color   | dates               |   height |\\n|----+-------+---------+---------------------+----------|\\n|  0 |     1 | blue    | 2019-03-06 00:00:00 |  2.72656 |\\n|  1 |     4 | blue    | 2019-03-05 00:00:00 |  4.77665 |\\n|  2 |     4 | green   | 2019-03-10 00:00:00 |  8.12169 |\\n|  3 |    10 | brown   | 2019-03-07 00:00:00 |  4.79977 |\\n|  4 |    20 | green   | 2019-03-01 00:00:00 |  3.92785 |\\n+----+-------+---------+---------------------+----------+\\n\\n\\nThis is a pandas dataframe provided to you above as input.\\n\\nYou need to transform it exactly to the following output dataframe:\\n\\n+----+----------+--------+---------+---------+---------+-------+----------+\\n|    | age      |   blue |   brown |   green |   month |   day |   height |\\n|----+----------+--------+---------+---------+---------+-------+----------|\\n|  0 | Under 18 |      1 |       0 |       0 |       3 |     6 |        3 |\\n|  1 | Under 18 |      1 |       0 |       0 |       3 |     5 |        5 |\\n|  2 | Under 18 |      0 |       0 |       1 |       3 |    10 |        8 |\\n|  3 | Under 18 |      0 |       1 |       0 |       3 |     7 |        5 |\\n|  4 | 18-25    |      0 |       0 |       1 |       3 |     1 |        4 |\\n+----+----------+--------+---------+---------+---------+-------+----------+\\n\\nYour code should be placed inside a function called transform_df that takes as input a dataframe and returns the transformed dataframe\\n', 'function_signature': \"\\nimport pandas as pd\\nfrom io import StringIO\\n\\n# Original dataset\\ndata = '''\\nage,color,dates,height\\n1,blue,2019-03-06,2.72656\\n4,blue,2019-03-05,4.77665\\n4,green,2019-03-10,8.12169\\n10,brown,2019-03-07,4.79977\\n20,green,2019-03-01,3.92785\\n'''\\n\\n# Read the dataset into a DataFrame\\ndf = pd.read_csv(StringIO(data))\\n\\ndef transform_df(df):\\n    # Your code here\\n\\nprint(transform_df(df))\\n\", 'unit_test': \"import numpy as np\\n\\ndata = '''\\nage,color,dates,height\\n1,blue,2019-03-06,2.72656\\n4,blue,2019-03-05,4.77665\\n4,green,2019-03-10,8.12169\\n10,brown,2019-03-07,4.79977\\n20,green,2019-03-01,3.92785\\n'''\\ndf = pd.read_csv(StringIO(data))\\nnew_data_testing = '''\\nage,blue,brown,green,month,day,height\\nUnder 18,1,0,0,3,6,3\\nUnder 18,1,0,0,3,5,5\\nUnder 18,0,0,1,3,10,8\\nUnder 18,0,1,0,3,7,5\\n18-25,0,0,1,3,1,4\\n'''\\ndef evaluate_correctness(target_df, input_df):\\n    target_df.reset_index(drop=True, inplace=True)\\n    input_df.reset_index(drop=True, inplace=True)\\n    score = 0\\n    for col in target_df.columns:\\n        if col not in input_df.columns:\\n            score -=1 \\n        else:\\n            if np.all(target_df[col] != input_df[col]):\\n                score-=1\\n    for col in input_df.columns:\\n        if col not in target_df.columns:\\n            score -=1\\n    return score\\nnew_data_df_testing = pd.read_csv(StringIO(new_data_testing))\\nassert evaluate_correctness(new_data_df_testing, transform_df(df)) == 0\\n\", 'solution': '\\nimport pandas as pd\\nfrom io import StringIO\\n\\ndef transform_df(df):\\n    # Process the dataset to match the desired format\\n    # 1. Convert \\'age\\' to categorical data\\n    df[\\'age\\'] = pd.cut(df[\\'age\\'], bins=[0, 18, 25, 100], labels=[\"Under 18\", \"18-25\", \"Over 25\"], right=False)\\n\\n    # 2. Convert \\'color\\' to one-hot encoding\\n    color_dummies = pd.get_dummies(df[\\'color\\'])\\n    df = pd.concat([df, color_dummies], axis=1)\\n\\n    # 3. Extract \\'month\\' and \\'day\\' from \\'dates\\'\\n    df[\\'dates\\'] = pd.to_datetime(df[\\'dates\\'])\\n    df[\\'month\\'] = df[\\'dates\\'].dt.month\\n    df[\\'day\\'] = df[\\'dates\\'].dt.day\\n\\n    # 4. Round \\'height\\' to nearest integer\\n    df[\\'height\\'] = df[\\'height\\'].round().astype(int)\\n\\n    # 5. Drop the original \\'color\\' and \\'dates\\' columns\\n    df.drop([\\'color\\', \\'dates\\'], axis=1, inplace=True)\\n\\n    # Rearrange columns to match the desired format\\n    df = df[[\\'age\\', \\'blue\\', \\'brown\\', \\'green\\', \\'month\\', \\'day\\', \\'height\\']]\\n    return df\\n', 'type': 'data_manip'},\n",
       "       {'name': 'is_multiply_prime', 'task_description': 'Write a function that returns true if the given number is the multiplication of 3 prime numbers and false otherwise. Knowing that (a) is less then 100. Example: is_multiply_prime(30) == True, 30 = 2 * 3 * 5', 'function_signature': 'def is_multiply_prime(a):', 'unit_test': 'assert is_multiply_prime(5) == False\\nassert is_multiply_prime(30) == True\\nassert is_multiply_prime(8) == True\\nassert is_multiply_prime(10) == False\\nassert is_multiply_prime(125) == True\\nassert is_multiply_prime(3 * 5 * 7) == True\\nassert is_multiply_prime(3 * 6 * 7) == False\\nassert is_multiply_prime(9 * 9 * 9) == False\\nassert is_multiply_prime(11 * 9 * 9) == False\\nassert is_multiply_prime(11 * 13 * 7) == True', 'solution': 'def is_multiply_prime(a):\\n    def is_prime(n):\\n        for j in range(2,n):\\n            if n%j == 0:\\n                return False\\n        return True\\n\\n    for i in range(2,101):\\n        if not is_prime(i): continue\\n        for j in range(2,101):\\n            if not is_prime(j): continue\\n            for k in range(2,101):\\n                if not is_prime(k): continue\\n                if i*j*k == a: return True\\n    return False', 'type': 'logic'},\n",
       "       {'name': 'tokenizer', 'task_description': \"\\nYour goal is to implement the build_vocabulary method in the provided Tokenizer class. \\nA tokenizer is an object that converts words to numerical IDs.\\n\\nThe objective of the build_vocabulary method is as follows:\\n\\n- The method's primary goal is to create two dictionaries: self.word_to_id and self.id_to_word.\\n\\n- self.word_to_id should map each unique word in your corpus to a unique numerical identifier (ID).\\n\\n- self.id_to_word is the reverse mapping, where each unique ID corresponds to a word.\\n\\n- The method should only consider the most frequent words in the corpus, up to a limit specified by max_vocab_size.\\n\\n\", 'function_signature': \"\\nclass Tokenizer:\\n    def __init__(self, max_vocab_size=200):\\n        self.max_vocab_size = max_vocab_size\\n        self.word_to_id = {}\\n        self.id_to_word = {}\\n\\n    def tokenize(self, text):\\n        # do not change\\n        # Split text into words by spaces\\n        return text.lower().split()\\n\\n    def build_vocabulary(self, corpus):\\n        '''\\n        corpus: a list of strings (string denotes a sentence composed of words seperated by spaces)\\n        '''\\n        # WRITE CODE HERE\\n        return \\n    \\n    def get_word_id(self, word):\\n        # do not change\\n        # Retrieve the ID of a word, return None if the word is not in the vocabulary\\n        return self.word_to_id.get(word)\\n\\n    def get_word_by_id(self, word_id):\\n        # do not change\\n        # Retrieve a word by its ID, return None if the ID is not in the vocabulary\\n        return self.id_to_word.get(word_id)\\n\", 'unit_test': '\\ndef test_tokenize():\\n    tokenizer = Tokenizer()\\n    assert tokenizer.tokenize(\"Hello world\") == [\"hello\", \"world\"], \"Tokenization failed\"\\n\\ndef test_build_vocabulary_and_get_word_id():\\n    tokenizer = Tokenizer(max_vocab_size=2)\\n    corpus = [\"hello world\", \"hello python\", \"hello world\"]\\n    tokenizer.build_vocabulary(corpus)\\n    \\n    assert tokenizer.get_word_id(\"hello\") is not None, \"\\'hello\\' should be in the vocabulary\"\\n    assert tokenizer.get_word_id(\"world\") is not None, \"\\'world\\' should be in the vocabulary\"\\n    assert tokenizer.get_word_id(\"python\") is None, \"\\'python\\' should not be in the vocabulary due to max_vocab_size limit\"\\n\\ndef test_get_word_by_id():\\n    tokenizer = Tokenizer(max_vocab_size=2)\\n    corpus = [\"apple orange\", \"banana apple\", \"cherry banana\"]\\n    tokenizer.build_vocabulary(corpus)\\n    \\n    apple_id = tokenizer.get_word_id(\"apple\")\\n    assert tokenizer.get_word_by_id(apple_id) == \"apple\", \"ID lookup for \\'apple\\' failed\"\\n\\n    # Assuming \\'cherry\\' is not in the top 2 words and therefore has no ID\\n    cherry_id = tokenizer.get_word_id(\"cherry\")\\n    assert cherry_id is None, \"\\'cherry\\' should not have an ID\"\\n    assert tokenizer.get_word_by_id(cherry_id) is None, \"ID lookup for a non-existent word should return None\"\\n\\n# Run the tests\\ntest_tokenize()\\ntest_build_vocabulary_and_get_word_id()\\ntest_get_word_by_id()\\n', 'solution': \"\\nfrom collections import Counter\\n\\nclass Tokenizer:\\n    def __init__(self, max_vocab_size=200):\\n        self.max_vocab_size = max_vocab_size\\n        self.word_to_id = {}\\n        self.id_to_word = {}\\n\\n    def tokenize(self, text):\\n        # Split text into words by spaces\\n        return text.lower().split()\\n\\n    def build_vocabulary(self, corpus):\\n        # to be implemented\\n        # Flatten the list of sentences into a list of words\\n        all_words = [word for sentence in corpus for word in self.tokenize(sentence)]\\n\\n        # Count the frequency of each word\\n        word_freq = Counter(all_words)\\n\\n        # Select the top 'max_vocab_size' words\\n        most_common_words = word_freq.most_common(self.max_vocab_size)\\n\\n        # Assign an ID to each word\\n        self.word_to_id = {word: idx for idx, (word, _) in enumerate(most_common_words)}\\n        self.id_to_word = {idx: word for word, idx in self.word_to_id.items()}\\n\\n    def get_word_id(self, word):\\n        # Retrieve the ID of a word, return None if the word is not in the vocabulary\\n        return self.word_to_id.get(word)\\n\\n    def get_word_by_id(self, word_id):\\n        # Retrieve a word by its ID, return None if the ID is not in the vocabulary\\n        return self.id_to_word.get(word_id)\\n\", 'type': 'edit_code'},\n",
       "       {'name': 't_test', 'task_description': '\\nYour goal is to complete the function simplified_t_test. This function takes as input two arrays of numbers and will return a float value called t_test. \\n\\nThe simplified_t_test is a statistical test that is used to compare the means of two populations. The value is computed as follows:\\n\\nt_test =  abs ( (mean1 - mean2) / sqrt((variance1 / n1) + (variance2 / n2))  )\\n\\nwhere mean1 and mean2 are the means of the two populations, variance1 and variance2 are the variances of the two populations with a modified denominator:\\nvariance1 = sum((x - mean1)^2) / (n1 - 2)\\nvariance2 = sum((x - mean2)^2) / (n2 - 2)\\n\\n, and n1 and n2 are the number of samples in each population. Note this is not the ordinary t-test, but a simplified version of it.\\n', 'function_signature': \"\\n\\n# function signature\\ndef simplified_t_test(sample1, sample2):\\n    '''\\n    :param sample1: List or array of sample data (sample 1)\\n    :param sample2: List or array of sample data (sample 2)\\n    :return: simplified t-test statistic\\n    '''\\n    t_test = 0\\n    # write your code here\\n    return t_test\\n\", 'unit_test': '\\nimport numpy as np\\n\\n# Test with known values\\nsample1 = [10, 20, 30, 40, 50]\\nsample2 = [30, 40, 50, 60, 70]\\nexpected_t_stat = 1.7320508075688774  # This value should be pre-calculated\\nprint(simplified_t_test(sample1, sample2))\\nassert np.isclose(simplified_t_test(sample1, sample2), expected_t_stat, atol=1e-3), \"Test with known values failed\"\\n\\n# Test with identical samples\\nidentical_sample = [1, 2, 3, 4, 5]\\nassert simplified_t_test(identical_sample, identical_sample) == 0, \"Test with identical samples failed\"\\n\\n\\nsample1 = [1,2,-1,3,4]\\nsample2 = [2,3,-2,4,5]\\nexpected_t_stat = 0.35032452487268523\\nprint(simplified_t_test(sample1, sample2))\\nassert np.isclose(simplified_t_test(sample1, sample2), expected_t_stat, atol=1e-3), \"Test with known values failed\"\\n', 'solution': \"\\nimport numpy as np\\n\\ndef simplified_t_test(sample1, sample2):\\n    '''\\n    :param sample1: List or array of sample data (sample 1)\\n    :param sample2: List or array of sample data (sample 2)\\n    :return: simplified t-test statistic\\n    '''\\n    t_test = 0\\n    # write your code here\\n    mean1 = np.mean(sample1)\\n    mean2 = np.mean(sample2)\\n    # variance with modified denominator\\n    variance1 = np.var(sample1, ddof=2)\\n    variance2 = np.var(sample2, ddof=2)\\n    n1 = len(sample1)\\n    n2 = len(sample2)\\n    t_test = (mean1 - mean2) / np.sqrt(variance1/n1 + variance2/n2)\\n    return abs(t_test)\\n\", 'type': 'data_manip_math'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tasks[tasks_chosen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting data for the tutorial task\n",
    "tutorial_task = all_tasks[tutorial_task_index]\n",
    "\n",
    "selected_tasks = [all_tasks[i] for i in tasks_chosen]\n",
    "\n",
    "# Creating the dictionary with the specified structure\n",
    "fb_taskgpt = {\n",
    "    'function_signatures': [task['function_signature'] for task in selected_tasks],\n",
    "    'task_descriptions': [task['task_description'] for task in selected_tasks],\n",
    "    'tutorial_function_signature': tutorial_task['function_signature'],\n",
    "    'model': \"gpt35\",  # As specified, for no model it is None\n",
    "    'tutorial_task_description': tutorial_task['task_description'],\n",
    "    'unit_tests': [task['unit_test'] for task in selected_tasks],\n",
    "    'exp_condition': 'pilot2',  # Placeholder, as the actual condition is not specified\n",
    "    'max_tokens': 64,  # Placeholder, adjust as necessary\n",
    "    'tutorial_unit_test': tutorial_task['unit_test'],\n",
    "    'id': \"pilot_gpt35\",\n",
    "    'task_completed': 0,\n",
    "}\n",
    "\n",
    "fb_taskllama = {\n",
    "    'function_signatures': [task['function_signature'] for task in selected_tasks],\n",
    "    'task_descriptions': [task['task_description'] for task in selected_tasks],\n",
    "    'tutorial_function_signature': tutorial_task['function_signature'],\n",
    "    'model': \"togethercomputer/CodeLlama-7b\",  # As specified, for no model it is None\n",
    "    'tutorial_task_description': tutorial_task['task_description'],\n",
    "    'unit_tests': [task['unit_test'] for task in selected_tasks],\n",
    "    'exp_condition': 'pilot2',  \n",
    "    'max_tokens': 64,  \n",
    "    'tutorial_unit_test': tutorial_task['unit_test'],\n",
    "    'id': \"pilot_llama7\",\n",
    "    'task_completed': 0,\n",
    "\n",
    "}\n",
    "\n",
    "fb_tasknomodel = {\n",
    "    'function_signatures': [task['function_signature'] for task in selected_tasks],\n",
    "    'task_descriptions': [task['task_description'] for task in selected_tasks],\n",
    "    'tutorial_function_signature': tutorial_task['function_signature'],\n",
    "    'model': \"togethercomputer/CodeLlama-7b\",  # As specified, for no model it is None\n",
    "    'tutorial_task_description': tutorial_task['task_description'],\n",
    "    'unit_tests': [task['unit_test'] for task in selected_tasks],\n",
    "    'exp_condition': 'pilot2',  \n",
    "    'max_tokens': 64,  \n",
    "    'tutorial_unit_test': tutorial_task['unit_test'],\n",
    "    'id': \"pilot_nomodel\",\n",
    "    'task_completed': 0,\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    doc_ref = db.collection(u'tasks').document(u'pilot_gpt35'+str(i))\n",
    "    doc_ref.set(fb_taskgpt)\n",
    "\n",
    "    doc_ref = db.collection(u'tasks').document(u'pilot_llama7'+str(i))\n",
    "    doc_ref.set(fb_taskllama)\n",
    "\n",
    "    doc_ref = db.collection(u'tasks').document(u'pilot_nomodel'+str(i))\n",
    "    doc_ref.set(fb_tasknomodel)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hussein2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
