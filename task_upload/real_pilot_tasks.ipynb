{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firebase signin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cred = credentials.Certificate(\"../codeinterface-85b5e-firebase-adminsdk-11q7e-837ba92a03.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.collection('tasks').get()\n",
    "# save docs in a pickle file called old_tasks.pkl\n",
    "tasks = {}\n",
    "for doc in docs:\n",
    "    doc_dict = doc.to_dict()\n",
    "    doc_dict['id'] = doc.id \n",
    "    tasks[doc.id] = doc_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date_string = str(np.datetime64('today', 'D'))\n",
    "with open('old_tasks'+today_date_string+'.pkl', 'wb') as f:\n",
    "    pickle.dump(tasks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the documents from the collection\n",
    "for doc in docs:\n",
    "    doc.reference.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = './tasks'  # Replace with the actual folder path\n",
    "\n",
    "# Initialize a list to store all tasks\n",
    "all_tasks = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Load JSON content\n",
    "            task_data = json.load(file)\n",
    "            # Add the task to the list\n",
    "            all_tasks.append(task_data)\n",
    "\n",
    "all_tasks = np.array(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find task in all_tasks with field name  is Average\n",
    "def get_task_index(task_name):\n",
    "    for i in range(len(all_tasks)):\n",
    "        if all_tasks[i]['name'] == task_name:\n",
    "            return i\n",
    "    # raise error if task not found\n",
    "    raise ValueError('Task not found')\n",
    "tutorial_task_index = get_task_index('sum_product')\n",
    "tasks_chosen = np.array([get_task_index('table_transform_named'), get_task_index('tokenizer')])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'name': 'table_transform_named', 'task_description': '\\n+----+-------+---------+---------------------+----------+\\n|    |   age | color   | dates               |   height |\\n|----+-------+---------+---------------------+----------|\\n|  0 |     1 | blue    | 2019-03-06 00:00:00 |  2.72656 |\\n|  1 |     4 | blue    | 2019-03-05 00:00:00 |  4.77665 |\\n|  2 |     4 | green   | 2019-03-10 00:00:00 |  8.12169 |\\n|  3 |    10 | brown   | 2019-03-07 00:00:00 |  4.79977 |\\n|  4 |    20 | green   | 2019-03-01 00:00:00 |  3.92785 |\\n+----+-------+---------+---------------------+----------+\\n\\n\\nThis is a pandas dataframe provided to you above as input.\\n\\nYou need to transform it exactly to the following output dataframe:\\n\\n+----+----------+--------+---------+---------+---------+-------+----------+\\n|    | age      |   blue |   brown |   green |   month |   day |   height |\\n|----+----------+--------+---------+---------+---------+-------+----------|\\n|  0 | Under 18 |      1 |       0 |       0 |       3 |     6 |        3 |\\n|  1 | Under 18 |      1 |       0 |       0 |       3 |     5 |        5 |\\n|  2 | Under 18 |      0 |       0 |       1 |       3 |    10 |        8 |\\n|  3 | Under 18 |      0 |       1 |       0 |       3 |     7 |        5 |\\n|  4 | 18-25    |      0 |       0 |       1 |       3 |     1 |        4 |\\n+----+----------+--------+---------+---------+---------+-------+----------+\\n\\nYour code should be placed inside a function called transform_df that takes as input a dataframe and returns the transformed dataframe\\n', 'function_signature': \"\\nimport pandas as pd\\nfrom io import StringIO\\n\\n# Original dataset\\ndata = '''\\nage,color,dates,height\\n1,blue,2019-03-06,2.72656\\n4,blue,2019-03-05,4.77665\\n4,green,2019-03-10,8.12169\\n10,brown,2019-03-07,4.79977\\n20,green,2019-03-01,3.92785\\n'''\\n\\n# Read the dataset into a DataFrame\\ndf = pd.read_csv(StringIO(data))\\n\\ndef transform_df(df):\\n    # Your code here\\n\\nprint(transform_df(df))\\n\", 'unit_test': \"\\n\\ndata = '''\\nage,color,dates,height\\n1,blue,2019-03-06,2.72656\\n4,blue,2019-03-05,4.77665\\n4,green,2019-03-10,8.12169\\n10,brown,2019-03-07,4.79977\\n20,green,2019-03-01,3.92785\\n'''\\ndf = pd.read_csv(StringIO(data))\\nnew_data_testing = '''\\nage,blue,brown,green,month,day,height\\nUnder 18,1,0,0,3,6,3\\nUnder 18,1,0,0,3,5,5\\nUnder 18,0,0,1,3,10,8\\nUnder 18,0,1,0,3,7,5\\n18-25,0,0,1,3,1,4\\n'''\\ndef evaluate_correctness(target_df, input_df):\\n    target_df.reset_index(drop=True, inplace=True)\\n    input_df.reset_index(drop=True, inplace=True)\\n    score = 0\\n    for col in target_df.columns:\\n        if col not in input_df.columns:\\n            score -=1 \\n        else:\\n            if np.all(target_df[col] != input_df[col]):\\n                score-=1\\n    for col in input_df.columns:\\n        if col not in target_df.columns:\\n            score -=1\\n    return score\\nnew_data_df_testing = pd.read_csv(StringIO(new_data_testing))\\nassert evaluate_correctness(new_data_df_testing, transform_df(df)) == 0\\n\", 'solution': '\\nimport pandas as pd\\nfrom io import StringIO\\n\\ndef transform_df(df):\\n    # Process the dataset to match the desired format\\n    # 1. Convert \\'age\\' to categorical data\\n    df[\\'age\\'] = pd.cut(df[\\'age\\'], bins=[0, 18, 25, 100], labels=[\"Under 18\", \"18-25\", \"Over 25\"], right=False)\\n\\n    # 2. Convert \\'color\\' to one-hot encoding\\n    color_dummies = pd.get_dummies(df[\\'color\\'])\\n    df = pd.concat([df, color_dummies], axis=1)\\n\\n    # 3. Extract \\'month\\' and \\'day\\' from \\'dates\\'\\n    df[\\'dates\\'] = pd.to_datetime(df[\\'dates\\'])\\n    df[\\'month\\'] = df[\\'dates\\'].dt.month\\n    df[\\'day\\'] = df[\\'dates\\'].dt.day\\n\\n    # 4. Round \\'height\\' to nearest integer\\n    df[\\'height\\'] = df[\\'height\\'].round().astype(int)\\n\\n    # 5. Drop the original \\'color\\' and \\'dates\\' columns\\n    df.drop([\\'color\\', \\'dates\\'], axis=1, inplace=True)\\n\\n    # Rearrange columns to match the desired format\\n    df = df[[\\'age\\', \\'blue\\', \\'brown\\', \\'green\\', \\'month\\', \\'day\\', \\'height\\']]\\n    return df\\n', 'type': 'data_manip'},\n",
       "       {'name': 'tokenizer', 'task_description': \"\\nYour goal is to implement the build_vocabulary method in the provided Tokenizer class. \\nA tokenizer is an object that converts words to numerical IDs.\\n\\nThe objective of the build_vocabulary method is as follows:\\n\\n- The method's primary goal is to create two dictionaries: self.word_to_id and self.id_to_word.\\n\\n- self.word_to_id should map each unique word in your corpus to a unique numerical identifier (ID).\\n\\n- self.id_to_word is the reverse mapping, where each unique ID corresponds to a word.\\n\\n- The method should only consider the most frequent words in the corpus, up to a limit specified by max_vocab_size.\\n\\n\", 'function_signature': \"\\nclass Tokenizer:\\n    def __init__(self, max_vocab_size=200):\\n        self.max_vocab_size = max_vocab_size\\n        self.word_to_id = {}\\n        self.id_to_word = {}\\n\\n    def tokenize(self, text):\\n        # do not change\\n        # Split text into words by spaces\\n        return text.lower().split()\\n\\n    def build_vocabulary(self, corpus):\\n        '''\\n        corpus: a list of strings (string denotes a sentence composed of words seperated by spaces)\\n        '''\\n        # WRITE CODE HERE\\n        return \\n    \\n    def get_word_id(self, word):\\n        # do not change\\n        # Retrieve the ID of a word, return None if the word is not in the vocabulary\\n        return self.word_to_id.get(word)\\n\\n    def get_word_by_id(self, word_id):\\n        # do not change\\n        # Retrieve a word by its ID, return None if the ID is not in the vocabulary\\n        return self.id_to_word.get(word_id)\\n\", 'unit_test': '\\ndef test_tokenize():\\n    tokenizer = Tokenizer()\\n    assert tokenizer.tokenize(\"Hello world\") == [\"hello\", \"world\"], \"Tokenization failed\"\\n\\ndef test_build_vocabulary_and_get_word_id():\\n    tokenizer = Tokenizer(max_vocab_size=2)\\n    corpus = [\"hello world\", \"hello python\", \"hello world\"]\\n    tokenizer.build_vocabulary(corpus)\\n    \\n    assert tokenizer.get_word_id(\"hello\") is not None, \"\\'hello\\' should be in the vocabulary\"\\n    assert tokenizer.get_word_id(\"world\") is not None, \"\\'world\\' should be in the vocabulary\"\\n    assert tokenizer.get_word_id(\"python\") is None, \"\\'python\\' should not be in the vocabulary due to max_vocab_size limit\"\\n\\ndef test_get_word_by_id():\\n    tokenizer = Tokenizer(max_vocab_size=2)\\n    corpus = [\"apple orange\", \"banana apple\", \"cherry banana\"]\\n    tokenizer.build_vocabulary(corpus)\\n    \\n    apple_id = tokenizer.get_word_id(\"apple\")\\n    assert tokenizer.get_word_by_id(apple_id) == \"apple\", \"ID lookup for \\'apple\\' failed\"\\n\\n    # Assuming \\'cherry\\' is not in the top 2 words and therefore has no ID\\n    cherry_id = tokenizer.get_word_id(\"cherry\")\\n    assert cherry_id is None, \"\\'cherry\\' should not have an ID\"\\n    assert tokenizer.get_word_by_id(cherry_id) is None, \"ID lookup for a non-existent word should return None\"\\n\\n# Run the tests\\ntest_tokenize()\\ntest_build_vocabulary_and_get_word_id()\\ntest_get_word_by_id()\\n', 'solution': \"\\nfrom collections import Counter\\n\\nclass Tokenizer:\\n    def __init__(self, max_vocab_size=200):\\n        self.max_vocab_size = max_vocab_size\\n        self.word_to_id = {}\\n        self.id_to_word = {}\\n\\n    def tokenize(self, text):\\n        # Split text into words by spaces\\n        return text.lower().split()\\n\\n    def build_vocabulary(self, corpus):\\n        # to be implemented\\n        # Flatten the list of sentences into a list of words\\n        all_words = [word for sentence in corpus for word in self.tokenize(sentence)]\\n\\n        # Count the frequency of each word\\n        word_freq = Counter(all_words)\\n\\n        # Select the top 'max_vocab_size' words\\n        most_common_words = word_freq.most_common(self.max_vocab_size)\\n\\n        # Assign an ID to each word\\n        self.word_to_id = {word: idx for idx, (word, _) in enumerate(most_common_words)}\\n        self.id_to_word = {idx: word for word, idx in self.word_to_id.items()}\\n\\n    def get_word_id(self, word):\\n        # Retrieve the ID of a word, return None if the word is not in the vocabulary\\n        return self.word_to_id.get(word)\\n\\n    def get_word_by_id(self, word_id):\\n        # Retrieve a word by its ID, return None if the ID is not in the vocabulary\\n        return self.id_to_word.get(word_id)\\n\", 'type': 'edit_code'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tasks[tasks_chosen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting data for the tutorial task\n",
    "tutorial_task = all_tasks[tutorial_task_index]\n",
    "\n",
    "selected_tasks = [all_tasks[i] for i in tasks_chosen]\n",
    "\n",
    "# Creating the dictionary with the specified structure\n",
    "fb_taskgpt = {\n",
    "    'function_signatures': [task['function_signature'] for task in selected_tasks],\n",
    "    'task_descriptions': [task['task_description'] for task in selected_tasks],\n",
    "    'tutorial_function_signature': tutorial_task['function_signature'],\n",
    "    'model': \"gpt35\",  # As specified, for no model it is None\n",
    "    'tutorial_task_description': tutorial_task['task_description'],\n",
    "    'unit_tests': [task['unit_test'] for task in selected_tasks],\n",
    "    'exp_condition': 'pilot2',  # Placeholder, as the actual condition is not specified\n",
    "    'max_tokens': 64,  # Placeholder, adjust as necessary\n",
    "    'tutorial_unit_test': tutorial_task['unit_test'],\n",
    "    'id': \"pilot_gpt35\",\n",
    "    'task_completed': 0,\n",
    "}\n",
    "\n",
    "fb_taskllama = {\n",
    "    'function_signatures': [task['function_signature'] for task in selected_tasks],\n",
    "    'task_descriptions': [task['task_description'] for task in selected_tasks],\n",
    "    'tutorial_function_signature': tutorial_task['function_signature'],\n",
    "    'model': \"togethercomputer/CodeLlama-7b\",  # As specified, for no model it is None\n",
    "    'tutorial_task_description': tutorial_task['task_description'],\n",
    "    'unit_tests': [task['unit_test'] for task in selected_tasks],\n",
    "    'exp_condition': 'pilot2',  \n",
    "    'max_tokens': 64,  \n",
    "    'tutorial_unit_test': tutorial_task['unit_test'],\n",
    "    'id': \"pilot_llama7\",\n",
    "    'task_completed': 0,\n",
    "\n",
    "}\n",
    "\n",
    "fb_tasknomodel = {\n",
    "    'function_signatures': [task['function_signature'] for task in selected_tasks],\n",
    "    'task_descriptions': [task['task_description'] for task in selected_tasks],\n",
    "    'tutorial_function_signature': tutorial_task['function_signature'],\n",
    "    'model': \"togethercomputer/CodeLlama-7b\",  # As specified, for no model it is None\n",
    "    'tutorial_task_description': tutorial_task['task_description'],\n",
    "    'unit_tests': [task['unit_test'] for task in selected_tasks],\n",
    "    'exp_condition': 'pilot2',  \n",
    "    'max_tokens': 64,  \n",
    "    'tutorial_unit_test': tutorial_task['unit_test'],\n",
    "    'id': \"pilot_nomodel\",\n",
    "    'task_completed': 0,\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    doc_ref = db.collection(u'tasks').document(u'pilot_gpt35'+str(i))\n",
    "    doc_ref.set(fb_taskgpt)\n",
    "\n",
    "    doc_ref = db.collection(u'tasks').document(u'pilot_llama7'+str(i))\n",
    "    doc_ref.set(fb_taskllama)\n",
    "\n",
    "    doc_ref = db.collection(u'tasks').document(u'pilot_nomodel'+str(i))\n",
    "    doc_ref.set(fb_tasknomodel)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hussein2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
