{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "\n",
    "cred = credentials.Certificate(\"../codeinterface-85b5e-firebase-adminsdk-11q7e-837ba92a03.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task structure in Firebase\n",
    "\n",
    "'function_signatures': list of strings,\n",
    "'task_descriptions': list of strings,\n",
    "'tutorial_function_signature': string,\n",
    "'model': string , for no model it is none\n",
    "'tutorial_task_description': string,\n",
    "'unit_tests': list of strings,\n",
    "'exp_condition': string,\n",
    "'max_tokens': int,\n",
    "'tutorial_unit_test': string,\n",
    "'id': string,\n",
    "\n",
    "Individual coding task structure and example:\n",
    "{\n",
    "    \"function_signature\": \"def average(nums)\",\n",
    "    \"task_description\": \"Write a Python function named average that calculates and returns the average of a list of numbers. The function should take one argument, nums, which is a list of numerical values. The average should be calculated as the sum of all elements in the list divided by the number of elements. If the list is empty, the function should return 0. \\n\\n The function signature should be: def average(nums)\",\n",
    "    \"unit_test\": \"assert average([1, 2.5, 3, 4.5]) == 2.75\\nassert average([-10, -20, -30]) == -20\\nassert average([]) == 0\"\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "docs = db.collection('tasks').get()\n",
    "tasks = {}\n",
    "for doc in docs:\n",
    "    doc_dict = doc.to_dict()\n",
    "    doc_dict['id'] = doc.id \n",
    "    tasks[doc.id] = doc_dict\n",
    "print(len(tasks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task1': {'function_signatures': ['def twoSum(nums, target):'],\n",
       "  'task_descriptions': ['Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. \\\\ \\\\n You may assume that each input would have exactly one solution, and you may not use the same element twice. \\\\ \\\\n You can return the answer in any order. \\\\n \\\\n  Required function signature: def twoSum(nums, target)'],\n",
       "  'tutorial_function_signature': 'def average(nums)',\n",
       "  'model': 'togethercomputer/CodeLlama-7b',\n",
       "  'tutorial_task_description': 'Write a Python function named average that calculates and returns the average of a list of numbers. The function should take one argument, nums, which is a list of numerical values. The average should be calculated as the sum of all elements in the list divided by the number of elements. If the list is empty, the function should return 0. \\\\n \\\\n The function signature should be: def average(nums)',\n",
       "  'unit_tests': ['assert twoSum([2,7,11,15], 9) == [0,1] \\\\nassert twoSum([3,2,4], 6) == [1,2] \\\\nassert twoSum([3,3], 6) == [0,1]'],\n",
       "  'exp_condition': 0,\n",
       "  'max_tokens': 20,\n",
       "  'tutorial_unit_test': 'assert average([1, 2.5, 3, 4.5]) == 2.75\\\\nassert average([-10, -20, -30]) == -20\\\\nassert average([]) == 0',\n",
       "  'id': 'task1'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "\n",
    "# sample 3 images from train for welcome screen\n",
    "welcome_images = random.sample(data_train_ids, 3)\n",
    "welcome_answers_raw = data_blurry['truths'][welcome_images]\n",
    "welcome_images =  bdd_dataset.image_paths[welcome_images]\n",
    "welcome_answers = []\n",
    "# format welcome_answers\n",
    "for i in range(0,len(welcome_answers_raw)):\n",
    "    if welcome_answers_raw[i] == 1:\n",
    "        welcome_answers.append(\"yes\")\n",
    "    else:\n",
    "        welcome_answers.append(\"no\")\n",
    "\n",
    "# data collection is 20 people, so need to create 20 sets of 40 images split 20-20\n",
    "TASKS_TOTAL = 20\n",
    "IMAGES_PER_TASK = 20\n",
    "ATTENTION_CHECKS = 2\n",
    "TASKS_PER_PERSON = 2\n",
    "tasks_data_collection = [] \n",
    "# sample image ids total without replacement\n",
    "images_data_collection = random.sample(data_train_ids, TASKS_TOTAL*IMAGES_PER_TASK*TASKS_PER_PERSON)\n",
    "# SHUFFLE\n",
    "random.shuffle(images_data_collection)\n",
    "for i in range(0,TASKS_TOTAL):\n",
    "    task_dict = {}\n",
    "    # first 20 images are with AI\n",
    "    ids_i = images_data_collection[i*IMAGES_PER_TASK*TASKS_PER_PERSON:(i+1)*IMAGES_PER_TASK*TASKS_PER_PERSON]\n",
    "    ids_with_ai = ids_i[0:IMAGES_PER_TASK]\n",
    "    ids_no_ai = ids_i[IMAGES_PER_TASK:IMAGES_PER_TASK*TASKS_PER_PERSON]\n",
    "    # attention check indicator\n",
    "    testing_withai_attentioncheck = np.zeros(IMAGES_PER_TASK)\n",
    "    testing_attentioncheck = np.zeros(IMAGES_PER_TASK)\n",
    "    # set attention check to 1 for 2 random locations\n",
    "    testing_withai_attentioncheck[random.sample(range(0, IMAGES_PER_TASK), ATTENTION_CHECKS)] = 1\n",
    "    testing_attentioncheck[random.sample(range(0, IMAGES_PER_TASK), ATTENTION_CHECKS)] = 1\n",
    "    # first do testing_images\n",
    "    testing_ai_answers_raw = data_blurry['preds'][ids_no_ai]\n",
    "    testing_ai_answers = []\n",
    "    testing_ai_scores = data_blurry['scores'][ids_no_ai]\n",
    "    # format testing_ai_answers\n",
    "    for i in range(0,len(testing_ai_answers_raw)):\n",
    "        new_answer = \"AI Predicts: \" \n",
    "        if testing_ai_answers_raw[i] == 1:\n",
    "            new_answer = new_answer + \"YES \"\n",
    "        else:\n",
    "            new_answer = new_answer + \"NO \"\n",
    "        new_answer = new_answer + \"with score: \" + str(round(testing_ai_scores[i],2))\n",
    "        testing_ai_answers.append(new_answer)\n",
    "\n",
    "    testing_labels = data_blurry['truths'][ids_no_ai]\n",
    "    testing_images = bdd_dataset.image_paths[ids_no_ai]\n",
    "    testing_ids = ids_no_ai\n",
    "    # then do testing_withai_images\n",
    "    testing_withai_ai_answers_raw = data_blurry['preds'][ids_with_ai]\n",
    "    testing_withai_ai_answers = []\n",
    "    testing_withai_ai_scores = data_blurry['scores'][ids_with_ai]\n",
    "    for i in range(0,len(testing_withai_ai_answers_raw)):\n",
    "        new_answer = \"AI Predicts: \" \n",
    "        if testing_withai_ai_answers_raw[i] == 1:\n",
    "            new_answer = new_answer + \"YES \"\n",
    "        else:\n",
    "            new_answer = new_answer + \"NO \"\n",
    "        new_answer = new_answer + \"with score: \" + str(round(testing_withai_ai_scores[i],2))\n",
    "        testing_withai_ai_answers.append(new_answer)\n",
    "    testing_withai_labels = data_blurry['truths'][ids_with_ai]\n",
    "    testing_withai_images = bdd_dataset.image_paths[ids_with_ai]\n",
    "    testing_withai_images_box = [testing_withai_images[i][:-4] + '_box.jpg' for i in range(0,len(testing_withai_images))]\n",
    "    testing_withai_ids = ids_with_ai\n",
    "\n",
    "    # add to task dict\n",
    "    task_dict['welcome_images'] = welcome_images.tolist()\n",
    "    task_dict['welcome_answers'] = welcome_answers\n",
    "    task_dict['testing_images'] = testing_images.tolist()\n",
    "    task_dict['testing_withai_images'] = testing_withai_images.tolist()\n",
    "    task_dict['testing_withai_ai_scores'] = testing_withai_ai_scores.tolist()\n",
    "    task_dict['testing_withai_ai_answers_raw'] = testing_withai_ai_answers_raw.tolist()\n",
    "    task_dict['testing_ai_answers_raw'] = testing_ai_answers_raw.tolist()\n",
    "    task_dict['testing_ai_scores'] = testing_ai_scores.tolist()\n",
    "    task_dict['testing_withai_images_box'] = testing_withai_images_box\n",
    "    task_dict['testing_withai_ai_answers'] = testing_withai_ai_answers\n",
    "    task_dict['testing_withai_label'] = testing_withai_labels.tolist()\n",
    "    task_dict['testing_withai_ids'] = testing_withai_ids\n",
    "    task_dict['testing_withai_attentioncheck'] = testing_withai_attentioncheck.tolist()\n",
    "    task_dict['testing_ai_answers'] = testing_ai_answers\n",
    "    task_dict['testing_labels'] = testing_labels.tolist()\n",
    "    task_dict['testing_ids'] = testing_ids\n",
    "    task_dict['testing_attentioncheck'] = testing_attentioncheck.tolist()\n",
    "    task_dict['exp_condition'] = 1\n",
    "    task_dict['data_collection'] = 1 \n",
    "\n",
    "    \n",
    "\n",
    "    # add to tasks list\n",
    "    tasks_data_collection.append(task_dict)\n",
    "\n",
    "\n",
    "# push to firebase\n",
    "for i in range(0,len(tasks_data_collection)):\n",
    "    # task id\n",
    "    task_id = \"task_\" + str(i) + \"exp_condition_1\"\n",
    "    doc_ref = db.collection(u'tasks').document(task_id)\n",
    "    doc_ref.set(tasks_data_collection[i]) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hussein2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
