{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re \n",
    "import ast \n",
    "import dateutil.parser as dparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_to_completion(telemetry_data):\n",
    "    starts = [event for event in telemetry_data if event[\"event_type\"] == \"load_task\"]\n",
    "    ends = [\n",
    "        event#[\"timestamp\"]\n",
    "        for event in telemetry_data\n",
    "        if event[\"event_type\"] == \"submit_code\" and event[\"completed_task\"] == 1\n",
    "    ]\n",
    "    times_tasks_solved = []\n",
    "    task_indices_seen = set()\n",
    "    for start in starts:\n",
    "        if start[\"task_index\"] == -1 or start[\"task_index\"] in task_indices_seen:\n",
    "            continue\n",
    "        task_indices_seen.add(start[\"task_index\"])\n",
    "        \n",
    "        for end in ends:\n",
    "            if end[\"task_index\"] == start[\"task_index\"]:\n",
    "                # check if tim is more than 10mins\n",
    "                if (end[\"timestamp\"] - start[\"timestamp\"]) / 1000 < 600:\n",
    "                    times_tasks_solved.append((end[\"timestamp\"] - start[\"timestamp\"]) / 1000)\n",
    "                    break\n",
    "\n",
    "\n",
    "    if len(times_tasks_solved) == 0:\n",
    "        return [], np.nan\n",
    "    return times_tasks_solved, np.mean(times_tasks_solved)\n",
    "\n",
    "\n",
    "def get_coding_time(telemetry_data):\n",
    "    # Get first load task\n",
    "    start = [event[\"timestamp\"] for event in telemetry_data if event[\"event_type\"] == \"load_task\"][0]\n",
    "\n",
    "    # Get last telemetry event\n",
    "    end = telemetry_data[-1][\"timestamp\"]\n",
    "\n",
    "    return (end - start) / 1000\n",
    "\n",
    "def get_time_verifying_suggestion(telemetry_data):\n",
    "    # Get suggestions\n",
    "    suggestions_shown = [event for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\"]\n",
    "\n",
    "    suggestions_reviewed = [\n",
    "        event for event in telemetry_data if event[\"event_type\"] == \"reject\" or event[\"event_type\"] == \"accept\"\n",
    "    ]\n",
    "\n",
    "    # Create a hashmap for suggestion reviews.\n",
    "    reviewed_hashmap = {}\n",
    "    for event in suggestions_reviewed:\n",
    "        reviewed_hashmap[event[\"suggestion_id\"]] = event[\"timestamp\"]\n",
    "\n",
    "    # Create a hashmap for times to completion\n",
    "    time_spent_verifying = {}\n",
    "    for event in suggestions_shown:\n",
    "        if event[\"suggestion_id\"] in reviewed_hashmap:\n",
    "            time_spent_verifying[event[\"suggestion_id\"]] = (\n",
    "                reviewed_hashmap[event[\"suggestion_id\"]] - event[\"timestamp\"]\n",
    "            ) / 1000\n",
    "        # else:\n",
    "        #     print(\"No review found for suggestion: \", event[\"suggestion_id\"])\n",
    "\n",
    "    return time_spent_verifying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = credentials.Certificate(\"../codeinterface-85b5e-firebase-adminsdk-11q7e-837ba92a03.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.collection('responses').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_TOOL_USAGE = {\"1\":\"Strongly Disagree\",\"2\":\"Disagree\", \"3\":\"Neutral\", \"4\":\"Agree\", \"5\":\"Strongly Agree\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15463/221858706.py:21: RuntimeWarning: Mean of empty slice\n",
      "  .assign(mean_task_duration = lambda x: [np.nanmean(y) for y in x.task_completion_durations])\n"
     ]
    }
   ],
   "source": [
    "df = (pd.DataFrame([x.to_dict() for x in docs])\n",
    "     .dropna(subset=[\"telemetry_data\", \"completed_task_time\", \"date_performed\"])\n",
    "     .assign(completed_task_time = lambda x: [dparser.parse(y, fuzzy=True) for y in x[\"completed_task_time\"]])\n",
    "     .assign(date_performed = lambda x: [dparser.parse(y, fuzzy=True) for y in x[\"date_performed\"]])\n",
    "     .assign(task_duration = lambda x: x.completed_task_time - x.date_performed)\n",
    "     .assign(model = lambda x: [re.match(\"[a-zA-Z]*_[a-zA-Z0-9]*\", x)[0] if re.match(\"[a-zA-Z]*_[a-zA-Z0-9]*\", str(x)) else \"\" for x in x[\"task_id\"]])\n",
    "     .assign(n_tasks_completed = lambda z: [len([x for x in y if x[\"event_type\"] == \"submit_code\" and x[\"completed_task\"] == 1 and x[\"task_index\"] != -1]) for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_tasks_attempted = lambda z: [len([x for x in y if x[\"event_type\"] == \"load_task\"]) for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_tasks_skipped = lambda z: [len([x for x in y if x[\"event_type\"] == \"skip_task\"]) for y in z[\"telemetry_data\"]])\n",
    "     .assign(TLX_frustration = lambda x: x[\"frustration\"].astype(int))\n",
    "     .assign(TLX_performance = lambda x: x[\"performance\"].astype(int))\n",
    "     .assign(TLX_temporal_demand = lambda x: x[\"temporalDemand\"].astype(int))\n",
    "     .assign(TLX_physical_demand = lambda x: x[\"physicalDemand\"].astype(int))\n",
    "     .assign(TLX_effort = lambda x: x[\"effort\"].astype(int))\n",
    "     .assign(TLX_mental_demand = lambda x: x[\"mentalDemand\"].astype(int))\n",
    "     .assign(TLX_total_score = lambda x: x.filter(like=\"TLX\").sum(axis=1) * 5)\n",
    "     .assign(n_sugg_accepted = lambda z: [len([x for x in y if x[\"event_type\"] == \"accept\"]) for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_sugg_shown = lambda z: [len([x for x in y if x[\"event_type\"] == \"suggestion_shown\" and x[\"suggestion\"] != \"\"]) for y in z[\"telemetry_data\"]])\n",
    "     .assign(sugg_accept_rate = lambda x: x.n_sugg_accepted / x.n_sugg_shown)\n",
    "     .assign(task_completion_durations = lambda x: [get_time_to_completion(y)[0] for y in x['telemetry_data']])\n",
    "     .assign(mean_task_duration = lambda x: [np.nanmean(y) for y in x.task_completion_durations])\n",
    "     .assign(coding_time = lambda x: [get_coding_time(y) for y in x['telemetry_data']])\n",
    "     .assign(time_spent_verifying = lambda x: [get_time_verifying_suggestion(y) for y in x['telemetry_data']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks\n",
    "* Add more after discussing data structure with Hussein "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This is failing ... what are these durations exactly? Are we comparing the right values?  \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_completion_durations\u001b[39m\u001b[38;5;124m\"\u001b[39m]]) \u001b[38;5;241m==\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_tasks_completed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is failing ... what are these durations exactly? Are we comparing the right values?  \n",
    "assert all(np.array([len(x) for x in df[\"task_completion_durations\"]]) == df[\"n_tasks_completed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>autocomplete_gpt35</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.569047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocomplete_llama34</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2.750</td>\n",
       "      <td>1.658312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocomplete_llama7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.587746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat_gpt35</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.25</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat_llama34</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.400</td>\n",
       "      <td>1.837873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat_llama7</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.483240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nomodel_0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.250</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.25</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nomodel_1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nomodel_2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nomodel_4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count   mean       std  min   25%  50%   75%  max\n",
       "model                                                                  \n",
       "autocomplete_gpt35     16.0  2.750  2.569047  0.0  0.00  2.5  5.00  7.0\n",
       "autocomplete_llama34   12.0  2.750  1.658312  0.0  2.00  2.5  3.25  6.0\n",
       "autocomplete_llama7     8.0  2.875  2.587746  0.0  0.75  2.5  5.00  7.0\n",
       "chat_gpt35             12.0  2.500  2.236068  0.0  1.00  2.0  4.25  7.0\n",
       "chat_llama34           10.0  2.400  1.837873  0.0  1.25  2.0  2.75  6.0\n",
       "chat_llama7            11.0  2.000  1.483240  0.0  1.00  2.0  2.50  5.0\n",
       "nomodel_0               4.0  4.250  1.500000  3.0  3.00  4.0  5.25  6.0\n",
       "nomodel_1               2.0  4.000  1.414214  3.0  3.50  4.0  4.50  5.0\n",
       "nomodel_2               2.0  2.500  0.707107  2.0  2.25  2.5  2.75  3.0\n",
       "nomodel_4               2.0  3.000  2.828427  1.0  2.00  3.0  4.00  5.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"model\")[\"n_tasks_completed\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 28 2024 20:34:44 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "It could be more contextualized to the problem at hand, and it could also integrate correct documentation.\n",
      "Mon Jan 29 2024 22:46:07 GMT-0800 (Pacific Standard Time)\n",
      "The feedback aspect was very poor. The output section for the code was very small 2 lines so I had to scroll to see my outputs and the error messages were difficult to parse. It would show me a assertion error but since it was pure text, I had a hard time seeing what was the correct output and what my output was. The question was sometimes hard. For the transforming dataframe part, the example shown was very out of shape and the rows and columns were not in line making it hard to read the dataframe.\n",
      "I wish I could prompt the AI to what I want to do, for example: If I make a comment in the program starting with AI like this, \"#AI What is the syntax for making a string upper case?\" Then the AI would answer this prompt with a comment. Another way AI could be improved is that if I could accept partial suggestions instead of accepting everything or disregarding suggestions. \n",
      "Mon Jan 29 2024 17:36:18 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "By ensuring that the suggestions meet the requirement and not change or add new functions that are not required.\n",
      "Mon Jan 29 2024 13:32:50 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "By changing the font color of suggestions provided by AI.\n",
      "Sun Jan 28 2024 21:40:32 GMT-0600 (Central Standard Time)\n",
      "The way the AI text was highlighted (maybe because of dark mode?) made it quite hard to read the AI generated text, which made it hard to tell if I wanted to use the suggestion.\n",
      "Suggestions after 2-seconds of inactivity seems bad. It would often interject an annoying amount of crap when I was just thinking, which I would have to remove. Only using the output when prompting for it was much more helpful.\n",
      "Mon Jan 29 2024 23:25:53 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "Although I mention that I could sort of develop on the \"hint\" prompted by the AI assistant in the previous question, I feel that most of the time, the suggestions given by it were not helpful at all. What is worse, I think the suggestion is misleading in someway, so I rejected most of its suggestion during this study.\n",
      "Mon Jan 29 2024 14:14:05 GMT-0500 (Eastern Standard Time)\n",
      "What a time and efforts \n",
      "More code suggestions \n",
      "Sun Jan 28 2024 19:49:50 GMT-0500 (Eastern Standard Time)\n",
      "Overall very frustrating to use the AI in this manner. I often wished that I could turn the AI at least while I thought instead of always having it on. I was also never sure what the AI was prompted on, I found my best success by prompting in comments and waiting for a the result, but even then I was looking for single line operations and it would give me multiple lines of code.\n",
      "\n",
      "It also kept trying to generate a main function even though there is no indication that it was needed. \n",
      "More time for the suggestion, or button that activates it. Sometimes while I was thinking about the solution, code would come up and interrupt my train of thought. I felt that I had to constantly check that the AI wasn't adding anything. \n",
      "\n",
      "I also didn't like that the AI's code looked like mine. This makes it hard to backtrack if there was a mistake since I forgot what I wrote and thought out compared to accidentally accepting the AI suggestion.\n",
      "\n",
      "Tue Jan 30 2024 03:14:31 GMT-0500 (Eastern Standard Time)\n",
      "NA\n",
      "Incorporating feedback from users to continuously improve the suggestions.\n",
      "Mon Jan 29 2024 11:59:42 GMT-0500 (Eastern Standard Time)\n",
      "None.\n",
      "The accuracy should be polished up! Please give the user the option to turn it off!\n",
      "Mon Jan 29 2024 00:28:06 GMT-0800 (Pacific Standard Time)\n",
      "AI suggestions need to improve the accuracy.\n",
      "Code suggestions should not be displayed all at once. Next suggestions should be made based on user input rather than trying to write out everything.\n",
      "Sun Jan 28 2024 18:53:56 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "It gave very fragmented code suggestions and wrong formulae not relating to the problems at hand. Copilot generally gives much better code suggestions and would have been more useful on this set.\n",
      "Sun Jan 28 2024 20:34:13 GMT-0500 (Eastern Standard Time)\n",
      "The interface was difficult to manage, I hope that can be improved.\n",
      "I felt the frequency of AI suggestions was too high as it severely impacted my ability to think about the question and the required logic.\n",
      "Most AI suggestions were not relevant to the task and printed irrelevant HTML code on several occasions.\n",
      "Sun Jan 28 2024 21:52:11 GMT-0500 (Eastern Standard Time)\n",
      "NA.\n",
      "I cannot say that it could be improved, because of how far off it was. Instead, it should be replaced by a significantly better assistant. It was very very far off from the correct answers and only hurt my performance.\n",
      "Mon Jan 29 2024 20:02:28 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "Maybe better model specifically good at coding tasks could be used?\n",
      "Mon Jan 29 2024 14:39:29 GMT-0500 (Eastern Standard Time)\n",
      "The code generation seems problematic. Nothing generated was useful at all\n",
      "something like copilot that generates actually relevant code would be better. also maybe a \"chat\" option where you can specify what you want it to do\n",
      "Mon Jan 29 2024 13:55:17 GMT-0500 (Eastern Standard Time)\n",
      " \n",
      "\n",
      "AI seemed to recommending empty blocks and text that was not python code. Maybe restrict it to the realm of code in the language that is being used?\n",
      "Mon Jan 29 2024 14:47:16 GMT-0500 (Eastern Standard Time)\n",
      "The AI assistant providing code every 2 seconds threw me off a great deal. I found it difficult to concentrate, and it was very difficult to make the suggestions go away.\n",
      "They should pertain to the task at hand a bit more, and be able to read the user's previous code to get a better grasp of what it should autofill with.\n",
      "Mon Jan 29 2024 20:05:23 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "Most of the time It generates random code (almost always in a different language too).\n",
      "It is unclear what actions lead to retaining the suggestions. Similar to other such tools the Tab key does, but there are other ways as well, since many times there was code in that I never intended to approve but it found its way without my knowledge.\n",
      "Mon Jan 29 2024 20:41:49 GMT-0500 (Eastern Standard Time)\n",
      "Completions were too frequent and were very distracting.\n",
      "Suggestions were not helpful for the most part.\n",
      "Mon Jan 29 2024 17:12:53 GMT+0530 (India Standard Time)\n",
      "\n",
      "\n",
      "Tue Jan 30 2024 09:45:29 GMT+0100 (West Africa Standard Time)\n",
      "Nil\n",
      "More accuracy so it can help improve skilss\n",
      "Sun Jan 28 2024 19:00:45 GMT-0500 (EST)\n",
      "Was finding it hard and wasn’t going through that much\n",
      "Bu being accurate and and giving relatable suggestions \n",
      "Mon Jan 29 2024 22:01:39 GMT-0500 (Eastern Standard Time)\n",
      "The experience of having the AI automatically give a suggestion after not typing for a bit was extremely annoying and distracting. Especially given the low quality of the AI.\n",
      "It had poor understanding of the context, often made syntax errors, made type errors, left code unfinished (even lines of code that it had wrote, some of them were just unfinished), and just generally did not understand what I wanted it to do (e.g. continue to write a comment even though I wanted it to start writing code).\n",
      "\n",
      "It seemed to weak to do any kind of reasoning. E.g. I tried to use it to compute a simple quantity, because I at first didn't realize how simple this was to compute: I wanted to take the string array of colors like ['brown', 'blue', 'brown'], and map it to an array with a 1 if it is brown, e.g. [1, 0, 1]. The AI made some complicated expression to do this that was wrong.\n",
      "\n",
      "Mon Jan 29 2024 00:33:54 GMT-0500 (Eastern Standard Time)\n",
      "prompts ambiguous.\n",
      "yes.  please see comments i left in my source code\n",
      "Mon Jan 29 2024 23:04:10 GMT-0500 (Eastern Standard Time)\n",
      "I think I was getting somehow annoyed by its continuously generating code. When I'm in the middle of typing but stop for a while, it might generate some non-sense code that would interfere with my current thinking, and I would have to spend extra energy reading the generated code to figure out what exactly those pieces of code are doing. This is taking a much greater amount of time than me working on the problem by myself. It's also possible that the AI is generating something useful but it's a different method than mine, so I also have to process that information and take time to determine what exactly I should do. \n",
      "Can we make the AI read the prompt too in a careful way such that it's less likely to generate some unuseful code? Also, auto generation with a high frequency might not be a great idea since when new things always pop out it's lowering the user's efficiency since we have to process that information at the same time too (before we decide whether we want to click on the tab or click on esc). Also after clicking on tab one time maybe don't do auto generation continuously since when it's filling up its own code again and again the generated things make less and less sense.\n",
      "Sun Jan 28 2024 22:18:49 GMT-0800 (Pacific Standard Time)\n",
      "The editor was a bit difficult to use; the code would randomly pop up without me expecting (in the middle of me typing, for example), and was often not what I wanted. It also sometimes erased what I had already written when I accidentally pressed tab. Being able to copy the prompt would have been really useful.\n",
      "Oftentimes disrupted my coding process (injecting code unnecessarily) or appeared to give completely irrelevant suggestions.\n",
      "Tue Jan 30 2024 02:36:37 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "Suggest correct responses that reflect what is desired. It was quite bad and also gave incomplete code\n",
      "Sun Jan 28 2024 18:20:57 GMT-0800 (Pacific Standard Time)\n",
      "\n",
      "Its debugging skills were not great at all and it gave me very unlikely errors\n",
      "Tue Jan 30 2024 15:21:10 GMT+0100 (West Africa Standard Time)\n",
      "\n",
      "better code related snippets\n",
      "Mon Jan 29 2024 17:41:20 GMT-0500 (Eastern Standard Time)\n",
      "The interaction is quite different from how I use AI for coding in real life. I mostly use copilot which allows me to write a comment and start coding and tries to fill the rest of it in. This study was more of an \"all or nothing\" setting where I either take all of it or none of it. I get that it might have been made to resemble ChatGPT more than CoPilot, but I still had to copy the whole suggestion and remove it which made me extremely not motivated to do the more fine-grained interactions I would with CoPilot or even chatgpt. \n",
      "Not sure whats meant by this. The quality of the predictions? They were adequate, I'd just like more fine-grained control as I describe in the next prompt. \n",
      "Sun Jan 28 2024 22:28:11 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "Better response\n",
      "Mon Jan 29 2024 19:25:35 GMT+0100 (West Africa Standard Time)\n",
      "The coding box was small couldn’t see my codes to know my error\n",
      "Bigger chat boxes to see all codes\n",
      "Mon Jan 29 2024 12:50:00 GMT-0500 (Eastern Standard Time)\n",
      "In one of the questions, pressing the next task button did not do anything. When I pressed the button a second time, it put me two questions ahead. So I think one of my answers (not the last) would be blank.\n",
      "Overall code output was never correct. This could be worked on\n",
      "Mon Jan 29 2024 20:09:47 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "Instead of writing complete programs, it would more useful to get a few lines of code as a specific answer to a question rather than a full working program for each question.\n",
      "Tue Jan 30 2024 11:35:05 GMT+0100 (West Africa Standard Time)\n",
      "\n",
      "it should be able to suggest where i missed it. \n",
      "Tue Jan 30 2024 00:03:17 GMT-0500 (Eastern Standard Time)\n",
      "Sum-digits-sort was confusing, since I didn't realize the negative sign counted. Would be useful if there was clarification of that. \n",
      "N/A, I thought it was very good and quick for the specific times I used it (though I didn't use it too much). \n",
      "Sun Jan 28 2024 18:54:29 GMT-0500 (Eastern Standard Time)\n",
      "The general interface could have been made more friendly and the instructions should include more details on how to run and submit code. I was trying to run the 1st problem for so long but wasn't seeing any output despite printing which was very confusing and finally ended up submitting and it turned out to be correct directly. I also Maybe using a traditional online platform might help reduce effort on that.\n",
      "If the AI was specifically meant for coding, I would expect it to give a code or pseudocode almost always in its response which was not there in the final questions as long as I didn't mention that I wanted code in python. Just the theoretical response doesn't really help in those cases. In a few output responses, there wasn;t proper syntax highlighting which I feel is necessary to make the output more easily readable instead of copying to a code editor and then checking.\n",
      "Mon Jan 29 2024 20:06:13 GMT-0500 (Eastern Standard Time)\n",
      "Definitely felt worse about my coding abilities, so that was fun. Clearly I rely on StackOverflow for wayyy too much.\n",
      "The AI suggestions themselves were pretty helpful. I don't use AI assistants for coding very regularly, so the fact that they import the proper libraries beforehand was cool to see.\n",
      "Sun Jan 28 2024 19:19:12 GMT-0500 (Eastern Standard Time)\n",
      "I have zero experience of using pandas. I can ask the AI how to generate code for the pandas task(s) but if things go wrong I would have no idea how to guide the AI to fix it.\n",
      "Its response is too verbose, and I'd like to read less. It would be better if it responded with fewer words but only elaborate if prompted.\n",
      "Mon Jan 29 2024 21:49:08 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "Some of the answers to my questions were incorrect. For example, one of the problems asked me to split a string by 3 different delimiters. The AI assistant claimed that I could use .split() with a string of all three delimiters, which did not work. For the question on sphenic numbers, the code provided by the AI chatbot incorrectly determined that 5 was a sphenic number. Furthermore, the responses are slow so sometimes it was faster to go off of my memory even if I wasn't sure if it would work.\n",
      "Mon Jan 29 2024 17:49:49 GMT-0800 (Pacific Standard Time)\n",
      "\n",
      "often gave me code not written in python (e.g. using {} like for in C or declearing variables with their type)\n",
      "Mon Jan 29 2024 20:13:10 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "better more accurate code\n",
      "Mon Jan 29 2024 11:45:45 GMT-0800 (Pacific Standard Time)\n",
      "Thank you for your time!\n",
      "I asked the AI a question like 2*2*23 or something like that, and it responded with a totally bogus answer like \"18\". That was pretty shocking and a little demoralizing to see because if it can't even figure out basic addition, what should I trust it for? \"Other\" AIs do a better job at avoiding gotchas like this.\n",
      "Sun Jan 28 2024 22:08:39 GMT-0800 (Pacific Standard Time)\n",
      "none\n",
      "its working  fine\n",
      "Mon Jan 29 2024 13:03:29 GMT+0100 (GMT+01:00)\n",
      "N/A\n",
      "The AI suggestions should work work more like GitHub copilot; providing solutions inside the development environment.\n",
      "Sun Jan 28 2024 20:01:27 GMT-0800 (Pacific Standard Time)\n",
      "the AI could not give suggestion of where an error is, it gives suggestion of anything that is entered even if it does not make sense.\n",
      "it should be able to give suggestions for possible errors and should be able to make adjustment based on the feedback received can help improve the quality of suggestions it could be by adjusting parameters, using different subsets of data, or even using different training algorithms.\n",
      "Mon Jan 29 2024 10:15:30 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "I like when it generated code examples because they were quicker to read than large blocks of text. \n",
      "Mon Jan 29 2024 22:52:02 GMT-0500 (Eastern Standard Time)\n",
      "This might be just me understanding the question incorrectly, but why is the assertion test for is_bored(\"Is the sky blue?\") set to one? Isn't it supposed to be 1 since it starts with 'I'? I wasted a lot of time trying to understand why this is the case.\n",
      "I think it should be able to quickly understand the previous context (previous conversation that I was already having with AI assistant) Honestly the conversation was so bad.\n",
      "Sun Jan 28 2024 19:02:03 GMT-0500 (Eastern Standard Time)\n",
      "\n",
      "It didn't carry forward the context so I couldn't do any debugging of the code it spit out in the beginning. \n",
      "Mon Jan 29 2024 22:16:32 GMT+0000 (Coordinated Universal Time)\n",
      "\n",
      "it should be able to include libraries i wasn't able to test that though, it should be able to able a whole mini project. it should also give suggestions or different options on how to code or solve your problem.\n",
      "Mon Jan 29 2024 00:11:36 GMT-0800 (Pacific Standard Time)\n",
      "None.\n",
      "Better long term reasoning. E.g. it forgot the function definitions for the calculator across two messages\n",
      "Sun Jan 28 2024 23:05:25 GMT-0500 (Eastern Standard Time)\n",
      "I took the test before. But I had missed the part about the AI. \n",
      "This is a retake. \n",
      "Also the IDE has a problem. Some solutions I tried on other environments and they worked fine but the one you gave me has a problem.\n",
      "Make it more context focused. \n",
      "I asked it about code and it gave me information about women’s emancipation\n",
      "Mon Jan 29 2024 20:03:56 GMT-0800 (Pacific Standard Time)\n",
      "\n",
      "none\n",
      "Mon Jan 29 2024 16:45:11 GMT-0600 (Central Standard Time)\n",
      "\n",
      "none\n",
      "Mon Jan 29 2024 15:04:32 GMT-0500 (Eastern Standard Time)\n",
      "When submitting code for testing, it would be nice if I could copy the test case that failed (the inputs and expected return) - I understand why copy/paste is disabled except for the contents from editor but it was a bit annoying to memorize a case, exit the alert, type down the input/expected pair and then do it again. \n",
      "none\n",
      "Mon Jan 29 2024 19:44:46 GMT-0500 (Eastern Standard Time)\n",
      "I felt hampered by the inability to do web searches for documentation, algorithms, etc.\n",
      "none\n",
      "Mon Jan 29 2024 22:11:47 GMT-0500 (Eastern Standard Time)\n",
      "I am not that good at Pandas, so that question took me a long time and I was not able to solve it.\n",
      "none\n",
      "Sun Jan 28 2024 18:48:09 GMT-0500 (Eastern Standard Time)\n",
      "The run output box was quite infuriating as it did not work as expected most of the time.\n",
      "none\n",
      "Mon Jan 29 2024 20:02:25 GMT-0500 (Eastern Standard Time)\n",
      "I do not have much pandas knowledge so that was difficult\n",
      "none\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "docs = db.collection('responses').get()\n",
    "# save docs in a pickle file called old_tasks.pkl\n",
    "responses = {}\n",
    "all_responses = {}\n",
    "done_count = 0\n",
    "for doc in docs:\n",
    "    doc_dict = doc.to_dict()\n",
    "    doc_dict['id'] = doc.id \n",
    "    all_responses[doc.id] = doc_dict\n",
    "    if 'entered_exit_survey' and 'finalcomments'  in doc_dict:\n",
    "        print(doc_dict['entered_exit_survey'])\n",
    "        responses[doc.id] = doc_dict\n",
    "\n",
    "        done_count += 1\n",
    "        print(doc_dict['finalcomments'])\n",
    "        print(doc_dict['howaiimproved'])\n",
    "\n",
    "\n",
    "\n",
    "print(done_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_responses, open('responses.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser as dparser\n",
    "\n",
    "\n",
    "def process_log(study_data, type = \"none\"):\n",
    "    frustration = int(study_data[\"frustration\"])\n",
    "    performance = int(study_data[\"performance\"])\n",
    "    temporal_demand = int(study_data[\"temporalDemand\"])\n",
    "    physical_demand = int(study_data[\"physicalDemand\"])\n",
    "    effort = int(study_data[\"effort\"])\n",
    "    mental_demand = int(study_data[\"mentalDemand\"])\n",
    "    tlx_score = get_tlx_score(frustration, performance, temporal_demand, physical_demand, effort, mental_demand)\n",
    "    #print(\"TLX Score: \", tlx_score)\n",
    "\n",
    "\n",
    "    completed_time = dparser.parse(study_data[\"completed_task_time\"], fuzzy=True)\n",
    "    date_performed = dparser.parse(study_data[\"date_performed\"], fuzzy=True)\n",
    "    study_completion = get_completion_time(date_performed, completed_time)\n",
    "    #print(\"Completion Time: \", study_completion)\n",
    "\n",
    "\n",
    "    tasks_completed = get_tasks_completed(study_data[\"telemetry_data\"])\n",
    "    #print(\"Tasks Completed: \", tasks_completed)\n",
    "\n",
    "    tasks_attempted = get_tasks_attempted(study_data[\"telemetry_data\"])\n",
    "    #print(\"Tasks Attempted: \", tasks_attempted)\n",
    "\n",
    "    time_to_completion, avg_time_to_completion = get_time_to_completion(study_data[\"telemetry_data\"])\n",
    "    #print(\"Time To Completion: \", time_to_completion)\n",
    "    #print(\"Average Time To Completion: \", avg_time_to_completion)\n",
    "\n",
    "    tasks_skipped = get_tasks_skipped(study_data[\"telemetry_data\"])\n",
    "    #print(\"Tasks Skipped: \", tasks_skipped)\n",
    "\n",
    "    coding_time = get_coding_time(study_data[\"telemetry_data\"])\n",
    "    #print(\"Coding Time: \", coding_time)\n",
    "    additional_metrics = {}\n",
    "    if type == \"autocomplete\":\n",
    "        accept_rate = get_suggestion_acceptance_rate(study_data[\"telemetry_data\"])\n",
    "        additional_metrics = {\n",
    "            \"accept_rate\": accept_rate,\n",
    "        }\n",
    "    \n",
    "    if type == \"chat\":\n",
    "        # count 'assistant_response', 'copy_code' event types in telemetry_data\n",
    "        assistant_response_count = len([event for event in study_data[\"telemetry_data\"] if event[\"event_type\"] == \"assistant_response\"])\n",
    "        copy_code_count = len([event for event in study_data[\"telemetry_data\"] if event[\"event_type\"] == \"copy_code\"])\n",
    "        paste_into_editor_count = len([event for event in study_data[\"telemetry_data\"] if event[\"event_type\"] == \"paste_into_editor\"])\n",
    "        additional_metrics = {\n",
    "            \"assistant_response_count\": assistant_response_count,\n",
    "            \"copy_code_count\": copy_code_count,\n",
    "            \"paste_into_editor_count\": paste_into_editor_count,\n",
    "        }\n",
    "    \t\n",
    "    if type == \"chat\" or type == \"autocomplete\":\n",
    "        # aihelpful \n",
    "        aihelpful = int(study_data[\"aihelpful\"])\n",
    "        additional_metrics[\"aihelpful\"] = aihelpful\n",
    "\n",
    "    dict_metrics  = {\n",
    "\n",
    "        \"tlx_score\": tlx_score,\n",
    "        \"study_completion\": study_completion,\n",
    "        \"tasks_completed\": tasks_completed,\n",
    "        \"tasks_attempted\": tasks_attempted,\n",
    "        \"time_to_completion\": time_to_completion,\n",
    "        \"avg_time_to_completion\": avg_time_to_completion,\n",
    "        \"tasks_skipped\": tasks_skipped,\n",
    "        \"coding_time\": coding_time,\n",
    "        **additional_metrics\n",
    "    }\n",
    "    return dict_metrics\n",
    "\n",
    "def get_tlx_score(frustration, performance, temporal_demand, physical_demand, effort, mental_demand):\n",
    "    return (frustration + performance + temporal_demand + physical_demand + effort + mental_demand) * 5\n",
    "\n",
    "\n",
    "def convert_tool_usage_to_str(tool_usage):\n",
    "    if tool_usage == \"1\":\n",
    "        return \"Strongly Disagree\"\n",
    "    elif tool_usage == \"2\":\n",
    "        return \"Disagree\"\n",
    "    elif tool_usage == \"3\":\n",
    "        return \"Neutral\"\n",
    "    elif tool_usage == \"4\":\n",
    "        return \"Agree\"\n",
    "    elif tool_usage == \"5\":\n",
    "        return \"Strongly Agree\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid tool usage\")\n",
    "\n",
    "\n",
    "def get_completion_time(start_time, end_time):\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def get_suggestion_acceptance_rate(telemetry_data):\n",
    "    num_accept = len([event for event in telemetry_data if event[\"event_type\"] == \"accept\"])\n",
    "    # only count suggestion_shown when suggestion is not \"\"\n",
    "    num_suggestion_shown = len([event for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"])\n",
    "    # if num_suggestion_shown == 0:\n",
    "    #     print(\"No suggestions shown!!\")\n",
    "    #     return np.nan\n",
    "    \n",
    "    return num_accept / num_suggestion_shown\n",
    "\n",
    "\n",
    "def get_tasks_completed(telemetry_data):\n",
    "    return len(\n",
    "        [event for event in telemetry_data if event[\"event_type\"] == \"submit_code\" and event[\"completed_task\"] == 1 and event[\"task_index\"] != -1]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tasks_attempted(telemetry_data):\n",
    "    return len([event for event in telemetry_data if event[\"event_type\"] == \"load_task\"])\n",
    "\n",
    "\n",
    "def get_time_to_completion(telemetry_data):\n",
    "    starts = [event for event in telemetry_data if event[\"event_type\"] == \"load_task\"]\n",
    "    ends = [\n",
    "        event#[\"timestamp\"]\n",
    "        for event in telemetry_data\n",
    "        if event[\"event_type\"] == \"submit_code\" and event[\"completed_task\"] == 1\n",
    "    ]\n",
    "    times_tasks_solved = []\n",
    "    task_indices_seen = set()\n",
    "    for start in starts:\n",
    "        if start[\"task_index\"] == -1 or start[\"task_index\"] in task_indices_seen:\n",
    "            continue\n",
    "        task_indices_seen.add(start[\"task_index\"])\n",
    "        \n",
    "        for end in ends:\n",
    "            if end[\"task_index\"] == start[\"task_index\"]:\n",
    "                # check if tim is more than 10mins\n",
    "                if (end[\"timestamp\"] - start[\"timestamp\"]) / 1000 < 600:\n",
    "                    times_tasks_solved.append((end[\"timestamp\"] - start[\"timestamp\"]) / 1000)\n",
    "                    break\n",
    "\n",
    "\n",
    "    if len(times_tasks_solved) == 0:\n",
    "        return 0, np.nan\n",
    "    return times_tasks_solved, np.mean(times_tasks_solved)\n",
    "\n",
    "\n",
    "def get_coding_time(telemetry_data):\n",
    "    # Get first load task\n",
    "    start = [event[\"timestamp\"] for event in telemetry_data if event[\"event_type\"] == \"load_task\"][0]\n",
    "\n",
    "    # Get last telemetry event\n",
    "    end = telemetry_data[-1][\"timestamp\"]\n",
    "\n",
    "    return (end - start) / 1000\n",
    "\n",
    "\n",
    "def get_tasks_skipped(telemetry_data):\n",
    "    return len([event for event in telemetry_data if event[\"event_type\"] == \"skip_task\"])\n",
    "\n",
    "\n",
    "def get_tasks_skipped(telemetry_data):\n",
    "    return len([event for event in telemetry_data if event[\"event_type\"] == \"skip_task\"])\n",
    "\n",
    "\n",
    "def get_time_verifying_suggestion(telemetry_data):\n",
    "    # Get suggestions\n",
    "    suggestions_shown = [event for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\"]\n",
    "\n",
    "    suggestions_reviewed = [\n",
    "        event for event in telemetry_data if event[\"event_type\"] == \"reject\" or event[\"event_type\"] == \"accept\"\n",
    "    ]\n",
    "\n",
    "    # Create a hashmap for suggestion reviews.\n",
    "    reviewed_hashmap = {}\n",
    "    for event in suggestions_reviewed:\n",
    "        reviewed_hashmap[event[\"suggestion_id\"]] = event[\"timestamp\"]\n",
    "\n",
    "    # Create a hashmap for times to completion\n",
    "    time_spent_verifying = {}\n",
    "    for event in suggestions_shown:\n",
    "        if event[\"suggestion_id\"] in reviewed_hashmap:\n",
    "            time_spent_verifying[event[\"suggestion_id\"]] = (\n",
    "                reviewed_hashmap[event[\"suggestion_id\"]] - event[\"timestamp\"]\n",
    "            ) / 1000\n",
    "        else:\n",
    "            print(\"No review found for suggestion: \", event[\"suggestion_id\"])\n",
    "\n",
    "    return time_spent_verifying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocomplete_gpt35_0_4-4767663\n",
      "name Ming Chong Lim email mingchol@andrew.cmu.edu completed 7 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'request_suggestion', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_gpt35_0_9-263157\n",
      "name San Shin email samshin0714@gmail.com completed 2 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'request_suggestion', 'reject', 'save_code', 'code_reset', 'accept', 'run_code'}\n",
      "autocomplete_gpt35_1_1-5457944\n",
      "name Shrikara Varna email svarna@andrew.cmu.edu completed 3 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_gpt35_1_10-7542461\n",
      "name Nirajan Koirala email nkoirala@nd.edu completed 3 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_gpt35_1_2-7342708\n",
      "name Eric Schneider email franz.eric.schneider@gmail.com completed 6 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'request_suggestion', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_gpt35_1_3-4590470\n",
      "name Jennifer Guo email gwyjennifer@gmail.com completed 5 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_gpt35_1_9-7410549\n",
      "autocomplete_gpt35_2_1-3570641\n",
      "name Alfredo Gomez email alfredogomez58@gmail.com completed 2 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_gpt35_2_5-2770345\n",
      "autocomplete_gpt35_2_7-2692440\n",
      "name Glenn Chentianye Xu email chentiax@andrew.cmu.edu completed 3 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'reject', 'save_code', 'code_reset', 'accept', 'run_code'}\n",
      "autocomplete_gpt35_3_6-9892547\n",
      "autocomplete_gpt35_4_0-1918441\n",
      "name Edward Jin email xdanieh1@gmail.com completed 7 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_llama34_0_10-263435\n",
      "name Ankit Shibusam email ashibusa@andrew.cmu.edu completed 2 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'request_suggestion', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_llama34_0_2-9102663\n",
      "name Dhruv Malik email maliknamya.20@dartmouth.edu completed 3 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_llama34_0_4-2363867\n",
      "name Arya Shah email aryas@andrew.cmu.edu completed 2 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'request_suggestion', 'reject', 'save_code', 'code_reset', 'accept', 'run_code'}\n",
      "autocomplete_llama34_1_8-5764720\n",
      "name Alan Zhu email yixuanz2@andrew.cmu.edu completed 6 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'request_suggestion', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_llama34_1_9-185578\n",
      "name Amanda Liu email lamanda@mit.edu completed 3 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_llama34_2_5-312886\n",
      "autocomplete_llama34_3_3-3509597\n",
      "name Akash Kannan email akashkan@cs.cmu.edu completed 4 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_llama34_3_5-9795183\n",
      "name Yiming Zhang email yimingz3@cs.cmu.edu completed 2 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'request_suggestion', 'reject', 'save_code', 'code_reset', 'accept', 'run_code'}\n",
      "autocomplete_llama34_4_6-8049061\n",
      "name RAZIK SINGH GREWAL email razikgrewal@gmail.com completed 2 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'reject', 'save_code', 'run_code'}\n",
      "never accepted sug for autocomplete_llama34_4_6-8049061 name RAZIK SINGH GREWAL email razikgrewal@gmail.com\n",
      "autocomplete_llama34_4_8-4977549\n",
      "autocomplete_llama7_0_7-8867477\n",
      "autocomplete_llama7_0_9-7547274\n",
      "name Derek Lim email dereklim@mit.edu completed 5 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'request_suggestion', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_llama7_1_7-9057190\n",
      "name Samuel Tenka email samtenka@umich.edu completed 5 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'reject', 'save_code', 'code_reset', 'run_code'}\n",
      "never accepted sug for autocomplete_llama7_1_7-9057190 name Samuel Tenka email samtenka@umich.edu\n",
      "autocomplete_llama7_3_10-5733710\n",
      "autocomplete_llama7_4_1-6139132\n",
      "name Lily Tsai email tslilyai@mit.edu completed 3 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'skip_task', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "autocomplete_llama7_4_3-942171\n",
      "name Eamon Tracey email etracey@nd.edu completed 7 tasks\n",
      "event types {'before_shown', 'load_task', 'submit_code', 'suggestion_shown', 'request_suggestion', 'reject', 'save_code', 'accept', 'run_code'}\n",
      "chat_gpt35_0_3-3477274\n",
      "name Tu Trinh email tutrinh@berkeley.edu completed 5 tasks\n",
      "event types {'load_task', 'submit_code', 'run_code', 'copy_code', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_gpt35_1_0-275928\n",
      "name marie hernandez email marietheresahernandez@gmail.com completed 2 tasks\n",
      "event types {'load_task', 'submit_code', 'skip_task', 'run_code', 'save_code', 'paste_into_editor', 'clear_chat'}\n",
      "never responded for chat_gpt35_1_0-275928 name marie hernandez email marietheresahernandez@gmail.com\n",
      "{'task_index': 2, 'are_copy_paste_equal': True, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706623003910, 'copied_text': 'user_credentials'}\n",
      "{'task_index': 2, 'are_copy_paste_equal': True, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706623048642, 'copied_text': 'hash(password)'}\n",
      "{'task_index': 2, 'are_copy_paste_equal': True, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706623163107, 'copied_text': 'username not in self.user_credentials'}\n",
      "{'task_index': 2, 'are_copy_paste_equal': True, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706623179744, 'copied_text': 'username not in self.user_credentials'}\n",
      "{'task_index': 2, 'are_copy_paste_equal': True, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706623203899, 'copied_text': 'self.user_credentials'}\n",
      "{'task_index': 2, 'are_copy_paste_equal': True, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706623275232, 'copied_text': 'authenticate_user'}\n",
      "{'task_index': 2, 'are_copy_paste_equal': True, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706623317330, 'copied_text': 'self.user_credentials[username]'}\n",
      "{'task_index': 2, 'are_copy_paste_equal': True, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706623332502, 'copied_text': 'new_password'}\n",
      "chat_gpt35_1_9-1775980\n",
      "name Vikramjeet Das email therealvikramjeet@gmail.com completed 5 tasks\n",
      "event types {'load_task', 'submit_code', 'run_code', 'copy_code', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_gpt35_2_0-3364490\n",
      "chat_gpt35_2_6-7315808\n",
      "chat_gpt35_2_6-8421218\n",
      "name Ved Dandekar email vdandeka@andrew.cmu.edu completed 4 tasks\n",
      "event types {'load_task', 'submit_code', 'skip_task', 'run_code', 'copy_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_gpt35_3_2-4680726\n",
      "chat_gpt35_4_6-1305267\n",
      "chat_gpt35_4_7-6980963\n",
      "name James Zhao email jjzhao2@andrew.cmu.edu completed 7 tasks\n",
      "event types {'load_task', 'submit_code', 'run_code', 'copy_code', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama34_0_9-9042837\n",
      "name Shrey Gupta email shrey2809@gmail.com completed 6 tasks\n",
      "event types {'load_task', 'submit_code', 'run_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama34_1_0-8925311\n",
      "name Patrick Tinsley email ptinsley@nd.edu completed 2 tasks\n",
      "event types {'cancel_request', 'load_task', 'submit_code', 'skip_task', 'run_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama34_1_4-8925494\n",
      "name Yishen Chen email cyt046@gmail.com completed 5 tasks\n",
      "event types {'load_task', 'submit_code', 'run_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama34_1_9-5956389\n",
      "name Lauren Lee email llee23@nd.edu completed 2 tasks\n",
      "event types {'load_task', 'submit_code', 'skip_task', 'run_code', 'copy_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama34_2_4-5414017\n",
      "chat_llama34_3_4-4079919\n",
      "chat_llama34_4_7-5075754\n",
      "name Julia Wang email julialujiawang@gmail.com completed 2 tasks\n",
      "event types {'load_task', 'submit_code', 'skip_task', 'run_code', 'copy_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama7_0_1-5418903\n",
      "name denmark leek email denmarkleek@gmail.com completed 2 tasks\n",
      "event types {'load_task', 'submit_code', 'run_code', 'save_code', 'paste_into_editor', 'clear_chat'}\n",
      "never responded for chat_llama7_0_1-5418903 name denmark leek email denmarkleek@gmail.com\n",
      "{'task_index': -1, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706506997233, 'copied_text': 'def sum_product(lst):\\r\\n    if not lst:\\r\\n        return (0, 1)\\r\\n\\r\\n    # Initialize sum and product to 0 and 1, respectively\\r\\n    total_sum = 0\\r\\n    total_product = 1\\r\\n\\r\\n    # Calculate sum and product\\r\\n    for num in lst:\\r\\n        total_sum += num\\r\\n        total_product *= num\\r\\n\\r\\n    return (total_sum, total_product)\\r\\n\\r\\n# Test cases\\r\\nprint(sum_product([]))         # Output: (0, 1)\\r\\nprint(sum_product([1, 2, 3, 4])) # Output: (10, 24)\\r\\n'}\n",
      "{'task_index': 0, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706507285320, 'copied_text': 'def even_odd_count(number):\\r\\n    if not isinstance(number, int):\\r\\n        raise ValueError(\"Input must be an integer\")\\r\\n\\r\\n    even_count = 0\\r\\n    odd_count = 0\\r\\n\\r\\n    # Convert the number to a string to iterate through each digit\\r\\n    for digit in str(abs(number)):\\r\\n        digit = int(digit)\\r\\n        if digit % 2 == 0:\\r\\n            even_count += 1\\r\\n        else:\\r\\n            odd_count += 1\\r\\n\\r\\n    return (even_count, odd_count)\\r\\n\\r\\n# Test cases\\r\\nprint(even_odd_count(-12))  # Output: (1, 1)\\r\\nprint(even_odd_count(123))  # Output: (1, 2)\\r\\n'}\n",
      "{'task_index': 1, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706507658290, 'copied_text': 'def triples_sum_to_zero(nums):\\r\\n    if len(nums) < 3:\\r\\n        return False\\r\\n\\r\\n    nums.sort()  # Sort the list to simplify the process\\r\\n\\r\\n    for i in range(len(nums) - 2):\\r\\n        left = i + 1\\r\\n        right = len(nums) - 1\\r\\n\\r\\n        while left < right:\\r\\n            current_sum = nums[i] + nums[left] + nums[right]\\r\\n\\r\\n            if current_sum == 0:\\r\\n                return True\\r\\n            elif current_sum < 0:\\r\\n                left += 1\\r\\n            else:\\r\\n                right -= 1\\r\\n\\r\\n    return False\\r\\n\\r\\n# Test cases\\r\\nprint(triples_sum_to_zero([1, 3, 5, 0]))          # Output: False\\r\\nprint(triples_sum_to_zero([1, 3, -2, 1]))         # Output: True\\r\\nprint(triples_sum_to_zero([1, 2, 3, 7]))          # Output: False\\r\\nprint(triples_sum_to_zero([2, 4, -5, 3, 9, 7]))   # Output: True\\r\\nprint(triples_sum_to_zero([1]))                   # Output: False\\r\\n'}\n",
      "{'task_index': 2, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706508379465, 'copied_text': 'import pandas as pd\\r\\n\\r\\ndef transform_df(input_df):\\r\\n    # Create an empty DataFrame with the desired output columns\\r\\n    output_df = pd.DataFrame(columns=[\\'ic\\', \\'data\\', \\'|| age |\\', \\'blue\\', \\'brown\\', \\'green\\', \\'month\\', \\'age,col\\'])\\r\\n\\r\\n    for index, row in input_df.iterrows():\\r\\n        new_row = {\\r\\n            \\'ic\\': index,\\r\\n            \\'data\\': row[\\'color\\'],\\r\\n            \\'|| age |\\': \\'Under 18\\' if row[\\'age\\'] < 18 else \\'18-25\\',\\r\\n            \\'blue\\': 1 if row[\\'color\\'] == \\'blue\\' else 0,\\r\\n            \\'brown\\': 1 if row[\\'color\\'] == \\'brown\\' else 0,\\r\\n            \\'green\\': 1 if row[\\'color\\'] == \\'green\\' else 0,\\r\\n            \\'month\\': row[\\'dates\\'].month,\\r\\n            \\'age,col\\': f\"{row[\\'age\\']},{row[\\'color\\']}\"\\r\\n        }\\r\\n        output_df = output_df.append(new_row, ignore_index=True)\\r\\n\\r\\n    return output_df\\r\\n\\r\\n# Example usage:\\r\\n# Assuming df is your input DataFrame\\r\\ninput_df = pd.DataFrame({\\r\\n    \\'age\\': [1, 4, 4, 10, 20],\\r\\n    \\'color\\': [\\'blue\\', \\'blue\\', \\'green\\', \\'brown\\', \\'green\\'],\\r\\n    \\'dates\\': [\\'2019-03-06\\', \\'2019-03-05\\', \\'2019-03-10\\', \\'2019-03-07\\', \\'2019-03-01\\']\\r\\n})\\r\\n\\r\\n# Convert \\'dates\\' column to datetime\\r\\ninput_df[\\'dates\\'] = pd.to_datetime(input_df[\\'dates\\'])\\r\\n\\r\\noutput_df = transform_df(input_df)\\r\\nprint(output_df)\\r\\n'}\n",
      "{'task_index': 2, 'event_type': 'paste_into_editor', 'messageAIindex': 0, 'timestamp': 1706508483193, 'copied_text': 'import pandas as pd\\r\\n\\r\\ndef transform_df(input_df):\\r\\n    # Create an empty DataFrame with the desired output columns\\r\\n    output_df = pd.DataFrame(columns=[\\'ic\\', \\'data\\', \\'|| age |\\', \\'blue\\', \\'brown\\', \\'green\\', \\'month\\', \\'age,col\\'])\\r\\n\\r\\n    for index, row in input_df.iterrows():\\r\\n        new_row = {\\r\\n            \\'ic\\': index,\\r\\n            \\'data\\': row[\\'color\\'],\\r\\n            \\'|| age |\\': \\'Under 18\\' if row[\\'age\\'] < 18 else \\'18-25\\',\\r\\n            \\'blue\\': 1 if row[\\'color\\'] == \\'blue\\' else 0,\\r\\n            \\'brown\\': 1 if row[\\'color\\'] == \\'brown\\' else 0,\\r\\n            \\'green\\': 1 if row[\\'color\\'] == \\'green\\' else 0,\\r\\n            \\'month\\': row[\\'dates\\'].month,\\r\\n            \\'age,col\\': f\"{row[\\'age\\']},{row[\\'color\\']}\"\\r\\n        }\\r\\n        output_df = output_df.append(new_row, ignore_index=True)\\r\\n\\r\\n    return output_df\\r\\n\\r\\n# Example usage:\\r\\n# Assuming df is your input DataFrame\\r\\ninput_df = pd.DataFrame({\\r\\n    \\'age\\': [1, 4, 4, 10, 20],\\r\\n    \\'color\\': [\\'blue\\', \\'blue\\', \\'green\\', \\'brown\\', \\'green\\'],\\r\\n    \\'dates\\': [\\'2019-03-06\\', \\'2019-03-05\\', \\'2019-03-10\\', \\'2019-03-07\\', \\'2019-03-01\\']\\r\\n})\\r\\n\\r\\n# Convert \\'dates\\' column to datetime\\r\\ninput_df[\\'dates\\'] = pd.to_datetime(input_df[\\'dates\\'])\\r\\n\\r\\noutput_df = transform_df(input_df)\\r\\nprint(output_df)\\r\\n'}\n",
      "chat_llama7_0_3-7389257\n",
      "chat_llama7_0_4-6317906\n",
      "name Dennis Mark email dutsinm@gmail.com completed 2 tasks\n",
      "event types {'load_task', 'submit_code', 'run_code', 'copy_code', 'code_reset', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama7_1_0-3727187\n",
      "chat_llama7_1_2-3792898\n",
      "name Yooni Choi email yooni.jy.choi@gmail.com completed 4 tasks\n",
      "event types {'load_task', 'submit_code', 'skip_task', 'run_code', 'copy_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama7_1_5-6600304\n",
      "name Satyapriya Krishna email spkrishnaofficial@gmail.com completed 3 tasks\n",
      "event types {'load_task', 'submit_code', 'skip_task', 'run_code', 'copy_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama7_2_0-9680599\n",
      "chat_llama7_2_2-4548890\n",
      "name Anshul Nasery email anasery@uw.edu completed 2 tasks\n",
      "event types {'load_task', 'submit_code', 'skip_task', 'run_code', 'copy_from_chat', 'save_code', 'user_message', 'paste_into_editor', 'clear_chat', 'assistant_response'}\n",
      "chat_llama7_3_0-5876494\n",
      "nomodel_0_1-1500243\n",
      "name Lanlana Kiratiwudhikul email lylakirati@gmail.com completed 3 tasks\n",
      "event types {'load_task', 'run_code', 'submit_code', 'save_code'}\n",
      "nomodel_0_5-4335761\n",
      "name Stephen McKay email mckay.sjm98@gmail.com completed 3 tasks\n",
      "event types {'load_task', 'run_code', 'submit_code', 'save_code'}\n",
      "nomodel_1_2-421988\n",
      "name Harshal Chamdal email harshalc@mit.edu completed 5 tasks\n",
      "event types {'load_task', 'run_code', 'submit_code', 'save_code'}\n",
      "nomodel_1_8-6633163\n",
      "name Kyle Johnsen email kjohnsen@gatech.edu completed 3 tasks\n",
      "event types {'load_task', 'run_code', 'submit_code', 'save_code'}\n",
      "nomodel_2_3-1987468\n",
      "name Vincent Tombari email vinnietombari@gmail.com completed 3 tasks\n",
      "event types {'load_task', 'submit_code', 'skip_task', 'save_code', 'run_code'}\n",
      "nomodel_4_1-7439581\n",
      "name Michael Yang email michael@yangm.tech completed 5 tasks\n",
      "event types {'load_task', 'run_code', 'submit_code', 'save_code'}\n",
      "nomodel_4_6-4261580\n"
     ]
    }
   ],
   "source": [
    "autoocomplete_gpt35_metrics = {'num_responses': 0, 'tlx_score': [], 'study_completion': [], 'tasks_completed': [], 'tasks_attempted': [], 'time_to_completion': [], 'avg_time_to_completion': [], 'tasks_skipped': [], 'coding_time': [], 'accept_rate': [], 'aihelpful': []}\n",
    "autocomplete_llama34_metrics = {'num_responses': 0, 'tlx_score': [], 'study_completion': [], 'tasks_completed': [], 'tasks_attempted': [], 'time_to_completion': [], 'avg_time_to_completion': [], 'tasks_skipped': [], 'coding_time': [], 'accept_rate': [], 'aihelpful': []}\n",
    "autocomplete_llama7_metrics = {'num_responses': 0, 'tlx_score': [], 'study_completion': [], 'tasks_completed': [], 'tasks_attempted': [], 'time_to_completion': [], 'avg_time_to_completion': [], 'tasks_skipped': [], 'coding_time': [], 'accept_rate': [], 'aihelpful': []}\n",
    "chat_gpt35_metrics = {'num_responses': 0, 'tlx_score': [], 'study_completion': [], 'tasks_completed': [], 'tasks_attempted': [], 'time_to_completion': [], 'avg_time_to_completion': [], 'tasks_skipped': [], 'coding_time': [], 'assistant_response_count': [], 'copy_code_count': [], 'paste_into_editor_count': [],  'aihelpful': []}\n",
    "chat_llama34_metrics = {'num_responses': 0, 'tlx_score': [], 'study_completion': [], 'tasks_completed': [], 'tasks_attempted': [], 'time_to_completion': [], 'avg_time_to_completion': [], 'tasks_skipped': [], 'coding_time': [], 'assistant_response_count': [], 'copy_code_count': [], 'paste_into_editor_count': [],  'aihelpful': []}\n",
    "chat_llama7_metrics = {'num_responses': 0, 'tlx_score': [], 'study_completion': [], 'tasks_completed': [], 'tasks_attempted': [], 'time_to_completion': [], 'avg_time_to_completion': [], 'tasks_skipped': [], 'coding_time': [], 'assistant_response_count': [], 'copy_code_count': [], 'paste_into_editor_count': [],  'aihelpful': []}\n",
    "nomodel_metrics = {'num_responses': 0, 'tlx_score': [], 'study_completion': [], 'tasks_completed': [], 'tasks_attempted': [], 'time_to_completion': [], 'avg_time_to_completion': [], 'tasks_skipped': [], 'coding_time': []}\n",
    "for resp in responses.values():\n",
    "    if 'entered_exit_survey' not in resp:\n",
    "        continue\n",
    "    resp_id = resp['id']\n",
    "    print(resp_id)\n",
    "    log_metrics = process_log(resp)\n",
    "    if log_metrics['tasks_completed'] < 2:\n",
    "        continue\n",
    "    print(f'name {resp[\"name\"]} email {resp[\"email\"]} completed {log_metrics[\"tasks_completed\"]} tasks')\n",
    "    # get event types\n",
    "    print(f'event types {set([event[\"event_type\"] for event in resp[\"telemetry_data\"]])}')\n",
    "    if 'autocomplete_gpt35' in resp_id:\n",
    "        autoocomplete_gpt35_metrics['num_responses'] += 1\n",
    "        log_metrics = process_log(resp, type=\"autocomplete\")\n",
    "        # merge dicts\n",
    "        for key in log_metrics:\n",
    "            autoocomplete_gpt35_metrics[key].append(log_metrics[key])\n",
    "        if log_metrics['accept_rate'] <0.001:\n",
    "            print(f'never accepted sug for {resp_id} name {resp[\"name\"]} email {resp[\"email\"]}')\n",
    "            \n",
    "    elif 'autocomplete_llama34' in resp_id:\n",
    "        autocomplete_llama34_metrics['num_responses'] += 1\n",
    "        log_metrics = process_log(resp, type=\"autocomplete\")\n",
    "        for key in log_metrics:\n",
    "            autocomplete_llama34_metrics[key].append(log_metrics[key])\n",
    "        if log_metrics['accept_rate'] <0.001:\n",
    "            print(f'never accepted sug for {resp_id} name {resp[\"name\"]} email {resp[\"email\"]}')\n",
    "    elif 'autocomplete_llama7' in resp_id:\n",
    "        autocomplete_llama7_metrics['num_responses'] += 1\n",
    "        log_metrics = process_log(resp, type=\"autocomplete\")\n",
    "        for key in log_metrics:\n",
    "            autocomplete_llama7_metrics[key].append(log_metrics[key])\n",
    "        if log_metrics['accept_rate'] <0.001:\n",
    "            print(f'never accepted sug for {resp_id} name {resp[\"name\"]} email {resp[\"email\"]}')\n",
    "    elif 'chat_gpt35' in resp_id:\n",
    "        chat_gpt35_metrics['num_responses'] += 1\n",
    "        log_metrics = process_log(resp, type=\"chat\")\n",
    "        for key in log_metrics:\n",
    "            chat_gpt35_metrics[key].append(log_metrics[key])\n",
    "        if log_metrics['assistant_response_count'] < 1:\n",
    "            print(f'never responded for {resp_id} name {resp[\"name\"]} email {resp[\"email\"]}')\n",
    "            # print all events with type paste_into_editor\n",
    "            for event in resp[\"telemetry_data\"]:\n",
    "                if event[\"event_type\"] == \"paste_into_editor\":\n",
    "                    print(event)\n",
    "    elif 'chat_llama34' in resp_id:\n",
    "        chat_llama34_metrics['num_responses'] += 1\n",
    "        log_metrics = process_log(resp, type=\"chat\")\n",
    "        for key in log_metrics:\n",
    "            chat_llama34_metrics[key].append(log_metrics[key])\n",
    "        if log_metrics['assistant_response_count'] < 1:\n",
    "            print(f'never responded for {resp_id} name {resp[\"name\"]} email {resp[\"email\"]}')\n",
    "            for event in resp[\"telemetry_data\"]:\n",
    "                if event[\"event_type\"] == \"paste_into_editor\":\n",
    "                    print(event)\n",
    "    elif 'chat_llama7' in resp_id:\n",
    "        chat_llama7_metrics['num_responses'] += 1\n",
    "        log_metrics = process_log(resp, type=\"chat\")\n",
    "        for key in log_metrics:\n",
    "            chat_llama7_metrics[key].append(log_metrics[key])\n",
    "        if log_metrics['assistant_response_count'] < 1:\n",
    "            print(f'never responded for {resp_id} name {resp[\"name\"]} email {resp[\"email\"]}')\n",
    "            for event in resp[\"telemetry_data\"]:\n",
    "                if event[\"event_type\"] == \"paste_into_editor\":\n",
    "                    print(event)\n",
    "    elif 'nomodel' in resp_id:\n",
    "        nomodel_metrics['num_responses'] += 1\n",
    "        log_metrics = process_log(resp)\n",
    "        for key in log_metrics:\n",
    "            nomodel_metrics[key].append(log_metrics[key])\n",
    "    else:\n",
    "        print('no model found for response ' + resp_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoocomplete_gpt35_metrics\n",
      "{'num_responses': 9, 'tlx_score': [265, 260, 375, 380, 340, 135, 350, 455, 285], 'study_completion': [datetime.timedelta(seconds=2006), datetime.timedelta(seconds=3162), datetime.timedelta(seconds=2240), datetime.timedelta(seconds=2265), datetime.timedelta(seconds=2284), datetime.timedelta(seconds=2545), datetime.timedelta(seconds=2456), datetime.timedelta(seconds=2316), datetime.timedelta(seconds=1634)], 'tasks_completed': [7, 2, 3, 3, 6, 5, 2, 3, 7], 'tasks_attempted': [8, 6, 6, 6, 8, 7, 5, 6, 8], 'time_to_completion': [[134.873, 226.801, 269.65, 261.069, 146.12, 155.127, 558.661], [511.013, 45.36], [226.867, 208.59], [292.668, 528.917, 356.693], [146.457, 208.015, 273.293, 378.161, 319.032, 331.631], [127.053, 508.808, 386.152, 249.752, 244.011], [175.612], [262.154, 429.33], [88.23, 124.503, 304.871, 186.677, 319.081, 352.976, 65.946]], 'avg_time_to_completion': [250.32871428571428, 278.18649999999997, 217.7285, 392.75933333333336, 276.09816666666666, 303.1552, 175.612, 345.74199999999996, 206.04057142857144], 'tasks_skipped': [0, 2, 1, 1, 0, 0, 1, 1, 0], 'coding_time': [1889.798, 2099.57, 2099.374, 2099.343, 2099.473, 2008.944, 2101.621, 2099.754, 1496.106], 'accept_rate': [0.14285714285714285, 0.2391304347826087, 0.06666666666666667, 0.08450704225352113, 0.2375, 0.16129032258064516, 0.13513513513513514, 0.2, 0.09836065573770492], 'aihelpful': [2, 6, 2, 5, 2, 2, 3, 1, 1]}\n",
      "autocomplete_llama34_metrics\n",
      "{'num_responses': 8, 'tlx_score': [430, 245, 290, 350, 285, 360, 295, 370], 'study_completion': [datetime.timedelta(seconds=2459), datetime.timedelta(seconds=2442), datetime.timedelta(seconds=2239), datetime.timedelta(seconds=2247), datetime.timedelta(seconds=2219), datetime.timedelta(seconds=2429), datetime.timedelta(seconds=2357), datetime.timedelta(seconds=2133)], 'tasks_completed': [2, 3, 2, 6, 3, 4, 2, 2], 'tasks_attempted': [5, 6, 6, 8, 5, 6, 4, 5], 'time_to_completion': [[389.776, 241.324], [111.46, 101.571], [191.884, 294.798], [124.771, 295.978, 279.915, 267.377, 188.61, 336.189], [435.738, 460.896, 531.301], [135.433, 372.294, 240.782], [151.623], [271.965, 377.765]], 'avg_time_to_completion': [315.55, 106.5155, 243.341, 248.80666666666664, 475.9783333333333, 249.50300000000001, 151.623, 324.865], 'tasks_skipped': [1, 1, 2, 0, 0, 0, 0, 1], 'coding_time': [2109.516, 2069.65, 2106.014, 2099.553, 2099.565, 2084.691, 1995.667, 2078.869], 'accept_rate': [0.02702702702702703, 0.0425531914893617, 0.045454545454545456, 0.04285714285714286, 0.014705882352941176, 0.09230769230769231, 0.020833333333333332, 0.0], 'aihelpful': [2, 2, 1, 2, 1, 2, 3, 3]}\n",
      "autocomplete_llama7_metrics\n",
      "{'num_responses': 4, 'tlx_score': [435, 200, 265, 260], 'study_completion': [datetime.timedelta(seconds=2563), datetime.timedelta(seconds=2359), datetime.timedelta(seconds=2264), datetime.timedelta(seconds=1809)], 'tasks_completed': [5, 5, 3, 7], 'tasks_attempted': [8, 7, 6, 8], 'time_to_completion': [[102.94, 99.055, 218.45, 371.923, 203.867], [95.944, 130.157, 402.356, 118.582], [102.983, 173.597, 490.105], [73.387, 156.459, 400.134, 295.495, 335.726, 146.56, 100.088]], 'avg_time_to_completion': [199.24699999999999, 186.75975, 255.5616666666667, 215.40699999999998], 'tasks_skipped': [1, 0, 1, 0], 'coding_time': [2099.1, 2095.958, 2100.1, 1655.988], 'accept_rate': [0.13513513513513514, 0.0, 0.2222222222222222, 0.05454545454545454], 'aihelpful': [2, 1, 4, 2]}\n",
      "chat_gpt35_metrics\n",
      "{'num_responses': 5, 'tlx_score': [280, 415, 235, 325, 295], 'study_completion': [datetime.timedelta(seconds=2164), datetime.timedelta(seconds=2234), datetime.timedelta(seconds=2423), datetime.timedelta(seconds=2270), datetime.timedelta(seconds=1864)], 'tasks_completed': [5, 2, 5, 4, 7], 'tasks_attempted': [7, 5, 7, 7, 8], 'time_to_completion': [[87.373, 180.008, 559.456, 296.13], [3.424, 2.069], [79.322, 240.386, 446.571, 269.935], [151.154, 370.972, 38.291, 498.78], [105.848, 380.866, 232.599, 202.772, 379.878, 299.753, 61.044]], 'avg_time_to_completion': [280.74175, 2.7465, 259.0535, 264.79925, 237.53714285714287], 'tasks_skipped': [0, 2, 0, 1, 0], 'coding_time': [2099.315, 1544.918, 2093.615, 2099.585, 1762.86], 'assistant_response_count': [13, 0, 13, 14, 3], 'copy_code_count': [1, 0, 8, 5, 1], 'paste_into_editor_count': [5, 8, 12, 16, 7], 'aihelpful': [4, 2, 7, 6, 9]}\n",
      "chat_llama34_metrics\n",
      "{'num_responses': 5, 'tlx_score': [275, 215, 365, 190, 240], 'study_completion': [datetime.timedelta(seconds=2636), datetime.timedelta(seconds=2381), datetime.timedelta(seconds=2307), datetime.timedelta(seconds=2366), datetime.timedelta(seconds=2243)], 'tasks_completed': [6, 2, 5, 2, 2], 'tasks_attempted': [8, 6, 7, 5, 5], 'time_to_completion': [[133.129, 107.213, 565.525, 495.237, 334.676, 183.257], [247.98, 387.104], [182.838, 313.063, 436.483, 266.836, 403.074], [417.491, 320.234], [270.383]], 'avg_time_to_completion': [303.17283333333336, 317.542, 320.4588, 368.86249999999995, 270.383], 'tasks_skipped': [0, 2, 0, 1, 1], 'coding_time': [2093.806, 2099.436, 2061.535, 2096.034, 2099.495], 'assistant_response_count': [23, 12, 10, 10, 29], 'copy_code_count': [0, 0, 0, 1, 2], 'paste_into_editor_count': [22, 16, 7, 1, 12], 'aihelpful': [8, 5, 6, 2, 5]}\n",
      "chat_llama7_metrics\n",
      "{'num_responses': 5, 'tlx_score': [495, 425, 285, 445, 270], 'study_completion': [datetime.timedelta(seconds=2231), datetime.timedelta(seconds=2685), datetime.timedelta(seconds=2311), datetime.timedelta(seconds=2185), datetime.timedelta(seconds=2325)], 'tasks_completed': [2, 2, 4, 3, 2], 'tasks_attempted': [4, 4, 7, 6, 6], 'time_to_completion': [[325.774, 309.257], [143.84, 120.203], [172.954, 308.413, 230.814, 246.511], [436.844, 83.835], [175.287, 354.59]], 'avg_time_to_completion': [317.5155, 132.0215, 239.673, 260.3395, 264.9385], 'tasks_skipped': [0, 0, 1, 1, 2], 'coding_time': [2098.459, 1989.205, 2098.324, 2099.607, 2099.531], 'assistant_response_count': [0, 5, 17, 24, 7], 'copy_code_count': [0, 6, 1, 3, 0], 'paste_into_editor_count': [5, 10, 13, 34, 8], 'aihelpful': [9, 10, 3, 2, 5]}\n",
      "nomodel_metrics\n",
      "{'num_responses': 6, 'tlx_score': [290, 340, 140, 335, 375, 330], 'study_completion': [datetime.timedelta(seconds=2143), datetime.timedelta(seconds=2127), datetime.timedelta(seconds=2248), datetime.timedelta(seconds=2167), datetime.timedelta(seconds=2154), datetime.timedelta(seconds=2160)], 'tasks_completed': [3, 3, 5, 3, 3, 5], 'tasks_attempted': [5, 5, 7, 5, 6, 7], 'time_to_completion': [[522.472, 206.188], [352.713, 283.936], [81.455, 169.399, 283.321, 402.833], [196.712, 591.239], [169.794, 300.968], [140.149, 194.769, 369.837, 217.646]], 'avg_time_to_completion': [364.33, 318.3245, 234.252, 393.9755, 235.38100000000003, 230.60025], 'tasks_skipped': [0, 0, 0, 0, 1, 0], 'coding_time': [2096.739, 2100.632, 2099.467, 2099.373, 2085.928, 2099.689]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print metrics for each\n",
    "print('autoocomplete_gpt35_metrics')\n",
    "print(autoocomplete_gpt35_metrics)\n",
    "print('autocomplete_llama34_metrics')\n",
    "print(autocomplete_llama34_metrics)\n",
    "print('autocomplete_llama7_metrics')\n",
    "print(autocomplete_llama7_metrics)\n",
    "print('chat_gpt35_metrics')\n",
    "print(chat_gpt35_metrics)\n",
    "print('chat_llama34_metrics')\n",
    "print(chat_llama34_metrics)\n",
    "print('chat_llama7_metrics')\n",
    "print(chat_llama7_metrics)\n",
    "print('nomodel_metrics')\n",
    "print(nomodel_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_time_to_completion\n",
      "autocomplete_gpt35_metrics\n",
      "271.73899841269844\n",
      "autocomplete_llama34_metrics\n",
      "264.5228125\n",
      "autocomplete_llama7_metrics\n",
      "214.24385416666667\n",
      "chat_gpt35_metrics\n",
      "208.97562857142862\n",
      "chat_llama34_metrics\n",
      "316.08382666666665\n",
      "chat_llama7_metrics\n",
      "242.8976\n",
      "nomodel_metrics\n",
      "296.14387500000004\n",
      "tasks_completed\n",
      "autocomplete_gpt35_metrics\n",
      "4.222222222222222\n",
      "autocomplete_llama34_metrics\n",
      "3.0\n",
      "autocomplete_llama7_metrics\n",
      "5.0\n",
      "chat_gpt35_metrics\n",
      "4.6\n",
      "chat_llama34_metrics\n",
      "3.4\n",
      "chat_llama7_metrics\n",
      "2.6\n",
      "nomodel_metrics\n",
      "3.6666666666666665\n",
      "tasks_skipped\n",
      "autocomplete_gpt35_metrics\n",
      "0.6666666666666666\n",
      "autocomplete_llama34_metrics\n",
      "0.625\n",
      "autocomplete_llama7_metrics\n",
      "0.5\n",
      "chat_gpt35_metrics\n",
      "0.6\n",
      "chat_llama34_metrics\n",
      "0.8\n",
      "chat_llama7_metrics\n",
      "0.8\n",
      "nomodel_metrics\n",
      "0.16666666666666666\n",
      "aihelpful\n",
      "autocomplete_gpt35_metrics\n",
      "2.6666666666666665\n",
      "autocomplete_llama34_metrics\n",
      "2.0\n",
      "autocomplete_llama7_metrics\n",
      "2.25\n",
      "chat_gpt35_metrics\n",
      "5.6\n",
      "chat_llama34_metrics\n",
      "5.2\n",
      "chat_llama7_metrics\n",
      "5.8\n"
     ]
    }
   ],
   "source": [
    "# get average avg_time_to_completion and tasks_completed\n",
    "print('avg_time_to_completion')\n",
    "print('autocomplete_gpt35_metrics')\n",
    "print(np.nanmean(autoocomplete_gpt35_metrics['avg_time_to_completion']))\n",
    "print('autocomplete_llama34_metrics')\n",
    "print(np.nanmean(autocomplete_llama34_metrics['avg_time_to_completion']))\n",
    "print('autocomplete_llama7_metrics')\n",
    "print(np.nanmean(autocomplete_llama7_metrics['avg_time_to_completion']))\n",
    "print('chat_gpt35_metrics')\n",
    "print(np.nanmean(chat_gpt35_metrics['avg_time_to_completion']))\n",
    "print('chat_llama34_metrics')\n",
    "print(np.nanmean(chat_llama34_metrics['avg_time_to_completion']))\n",
    "print('chat_llama7_metrics')\n",
    "print(np.nanmean(chat_llama7_metrics['avg_time_to_completion']))\n",
    "print('nomodel_metrics')\n",
    "print(np.nanmean(nomodel_metrics['avg_time_to_completion']))\n",
    "\n",
    "print('tasks_completed')\n",
    "print('autocomplete_gpt35_metrics')\n",
    "print(np.nanmean(autoocomplete_gpt35_metrics['tasks_completed']))\n",
    "print('autocomplete_llama34_metrics')\n",
    "print(np.nanmean(autocomplete_llama34_metrics['tasks_completed']))\n",
    "print('autocomplete_llama7_metrics')\n",
    "print(np.nanmean(autocomplete_llama7_metrics['tasks_completed']))\n",
    "print('chat_gpt35_metrics')\n",
    "print(np.nanmean(chat_gpt35_metrics['tasks_completed']))\n",
    "print('chat_llama34_metrics')\n",
    "print(np.nanmean(chat_llama34_metrics['tasks_completed']))\n",
    "print('chat_llama7_metrics')\n",
    "print(np.nanmean(chat_llama7_metrics['tasks_completed']))\n",
    "print('nomodel_metrics')\n",
    "print(np.nanmean(nomodel_metrics['tasks_completed']))\n",
    "print(\"tasks_skipped\")\n",
    "print('autocomplete_gpt35_metrics')\n",
    "print(np.nanmean(autoocomplete_gpt35_metrics['tasks_skipped']))\n",
    "print('autocomplete_llama34_metrics')\n",
    "print(np.nanmean(autocomplete_llama34_metrics['tasks_skipped']))\n",
    "print('autocomplete_llama7_metrics')\n",
    "print(np.nanmean(autocomplete_llama7_metrics['tasks_skipped']))\n",
    "print('chat_gpt35_metrics')\n",
    "print(np.nanmean(chat_gpt35_metrics['tasks_skipped']))\n",
    "print('chat_llama34_metrics')\n",
    "print(np.nanmean(chat_llama34_metrics['tasks_skipped']))\n",
    "print('chat_llama7_metrics')\n",
    "print(np.nanmean(chat_llama7_metrics['tasks_skipped']))\n",
    "print('nomodel_metrics')\n",
    "print(np.nanmean(nomodel_metrics['tasks_skipped']))\n",
    "\n",
    "print(\"aihelpful\")\n",
    "print('autocomplete_gpt35_metrics')\n",
    "print(np.nanmean(autoocomplete_gpt35_metrics['aihelpful']))\n",
    "print('autocomplete_llama34_metrics')\n",
    "print(np.nanmean(autocomplete_llama34_metrics['aihelpful']))\n",
    "print('autocomplete_llama7_metrics')\n",
    "print(np.nanmean(autocomplete_llama7_metrics['aihelpful']))\n",
    "print('chat_gpt35_metrics')\n",
    "print(np.nanmean(chat_gpt35_metrics['aihelpful']))\n",
    "print('chat_llama34_metrics')\n",
    "print(np.nanmean(chat_llama34_metrics['aihelpful']))\n",
    "print('chat_llama7_metrics')\n",
    "print(np.nanmean(chat_llama7_metrics['aihelpful']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
